{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation de bibliotheques \n",
    "\n",
    "**pip install beautifulsoup4**\n",
    "\n",
    "**pip install requests**\n",
    "\n",
    "**pip install PyPDF2**\n",
    "\n",
    "**pip install pdfplumber**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pdfplumber\n",
    "import re, io\n",
    "import PyPDF2\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "import time\n",
    "from threading import Timer\n",
    "\n",
    "import logging\n",
    "import urllib.parse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraper les articles bioRxiv liés à la bioinformatique "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parcourir les articles de la collection Bioinformatics , scraper 10 pages à chaque exécution . (run toutes les semaines puisque new articles se mette en page 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'https://www.biorxiv.org'\n",
    "\n",
    "# Créer la base de données SQLite \"bioinformatics_articles\"\n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor()\n",
    "\n",
    "# Créer la table 'articles' si elle n'existe pas\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS articles\n",
    "             (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "             title TEXT, \n",
    "             link TEXT,\n",
    "             doi TEXT UNIQUE,\n",
    "             date TEXT,\n",
    "             pdf_link TEXT,\n",
    "             abstract TEXT)''')\n",
    "\n",
    "# Nombre de pages à scraper (exemple : 10 pages à la fois)\n",
    "total_pages = 10\n",
    "\n",
    "# Boucle qui parcourt chaque page de la collection Bioinformatics\n",
    "for page in range(1, total_pages + 1):\n",
    "    print(f\"Scraping page {page} sur {total_pages}\")\n",
    "\n",
    "    # Requête qui récupère la page bioinfo actuelle\n",
    "    response = requests.get(f'{url}/collection/bioinformatics?page={page}')\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Trouver les liens vers les articles sur cette page\n",
    "    articles = soup.find_all('a', class_='highwire-cite-linked-title')\n",
    "\n",
    "    print(f'Nombre total d\\'articles trouvés sur la page {page} : {len(articles)}')\n",
    "\n",
    "    # Parcourir tous les articles de la page et enregistrer leurs titres et liens\n",
    "    for article in articles:\n",
    "        title = article.text.strip()\n",
    "        link = url + article['href']\n",
    "\n",
    "        # Requête pour accéder à la page de l'article\n",
    "        article_response = requests.get(link)\n",
    "        article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "\n",
    "        # Extraire le DOI\n",
    "        doi_tag = article_soup.find('meta', {'name': 'citation_doi'})\n",
    "        doi = doi_tag['content'] if doi_tag else 'DOI non disponible'\n",
    "\n",
    "        # Extraire la date de publication\n",
    "        date_tag = article_soup.find('meta', {'name': 'citation_publication_date'})\n",
    "        date = date_tag['content'] if date_tag else 'Date non disponible'\n",
    "        if date != 'Date non disponible':\n",
    "            date = date.replace('/', '-')  # Remplacer '/' par '-'\n",
    "            date = datetime.strptime(date, '%Y-%m-%d').date().isoformat()\n",
    "\n",
    "        # Extraire le lien vers le fichier PDF\n",
    "        pdf_links = article_soup.find_all('a')\n",
    "        pdf_link = next((url + pdf['href'] for pdf in pdf_links if 'PDF' in pdf.text and pdf['href'].endswith('.pdf')), 'Lien PDF non disponible')\n",
    "\n",
    "        # Extraire l'abstract\n",
    "        abstract_tag = article_soup.find('meta', {'name': 'citation_abstract'})\n",
    "        clean_abstract = abstract_tag['content'] if abstract_tag else 'Abstract non disponible'\n",
    "\n",
    "        # Supprimer les balises <p> au début et à la fin de l'abstract\n",
    "        if clean_abstract.startswith('<p>'):\n",
    "            clean_abstract = clean_abstract[3:]  # Supprimer les 3 premiers caractères\n",
    "        if clean_abstract.endswith('</p>'):\n",
    "            clean_abstract = clean_abstract[:-4]  # Supprimer les 4 derniers caractères\n",
    "\n",
    "        # Insérer dans la base de données SQLite\n",
    "        c.execute(\"SELECT * FROM articles WHERE doi = ? AND date = ?\", (doi, date))\n",
    "        result = c.fetchone()\n",
    "\n",
    "        if result:\n",
    "            print(f\"L'article avec DOI {doi} existe déjà, ignoré.\")\n",
    "        else:\n",
    "            c.execute(\"INSERT INTO articles (title, link, doi, date, pdf_link, abstract) VALUES (?, ?, ?, ?, ?, ?)\", \n",
    "                      (title, link, doi, date, pdf_link, clean_abstract))\n",
    "            print(f\"Article ajouté : {title}\")\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Sauvegarder les modifications dans la base de données après chaque page\n",
    "    database.commit()\n",
    "    \n",
    "database.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extraction des URLs des Dépôts Logiciels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1.\tVérification et ajout des colonnes nécessaires dans la base de données.\n",
    "\n",
    "2.\tValidation des liens pour s’assurer qu’ils sont valides et pertinents.\n",
    "\n",
    "3.\tVérification de l’accessibilité des URLs via des requêtes HTTP.\n",
    "\n",
    "4.\tExtraction des URLs des dépôts à partir des contenus des PDF et des abstracts.\n",
    "\n",
    "5.\tTraitement des nouveaux articles non encore traités pour l’extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les articles qui ont été vérifiés, mais sans trouver de liens logiciels, seront mis à jour avec is_article_processed = 2. Cela permet de ne pas les vérifier à nouveau lors des exécutions futures.---> Le script ne traite que les articles où is_article_processed = 0. Cela évite de revérifier les articles déjà traités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Connexion à la base de données SQLite\n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor()\n",
    "\n",
    "# Vérifier et ajouter les colonnes software_links et is_article_processed si elles n'existent pas\n",
    "try:\n",
    "    c.execute(\"ALTER TABLE articles ADD COLUMN software_links TEXT\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass  # La colonne existe déjà\n",
    "\n",
    "try:\n",
    "    c.execute(\"ALTER TABLE articles ADD COLUMN is_article_processed INTEGER DEFAULT 0\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass  # La colonne existe déjà\n",
    "\n",
    "# Fonction pour vérifier si un lien est valide et s'il est un lien GitHub ou GitLab\n",
    "def is_valid_link(link):\n",
    "    return link.startswith('http://') or link.startswith('https://')\n",
    "\n",
    "def is_github_or_gitlab(link):\n",
    "    return 'github.com' in link or 'gitlab.com' in link\n",
    "\n",
    "# Fonction pour nettoyer les liens partiels ou incorrects\n",
    "def clean_link(link):\n",
    "    link = link.strip(').;,[]')  # Supprimer les caractères indésirables\n",
    "    if is_valid_link(link) and len(link) > len('https://github.com/'):\n",
    "        return link\n",
    "    return None\n",
    "\n",
    "# Fonction pour tester la validité des URLs via une requête HTTP\n",
    "def check_url_validity(link):\n",
    "    try:\n",
    "        response = requests.get(link, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Le lien est valide : {link}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Le lien a retourné un code de statut {response.status_code} pour : {link}\")\n",
    "            return False\n",
    "    except requests.RequestException:\n",
    "        print(f\"Erreur lors de la vérification du lien : {link}\")\n",
    "        return False\n",
    "\n",
    "# Fonction pour extraire les URLs des dépôts logiciels\n",
    "def extract_repository_urls(text):\n",
    "    url_pattern = r'(https?://[^\\s]+)'\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    repo_urls = []\n",
    "\n",
    "    for url in urls:\n",
    "        cleaned_url = clean_link(url)\n",
    "        if cleaned_url and is_github_or_gitlab(cleaned_url):\n",
    "            if check_url_validity(cleaned_url):  # Vérification de l'URL avant de l'ajouter\n",
    "                repo_urls.append(cleaned_url)\n",
    "\n",
    "    return repo_urls\n",
    "\n",
    "# Fonction pour extraire les liens de dépôt logiciel\n",
    "def extract_software_links(article_id, pdf_link, abstract):\n",
    "    software_links_set = set()\n",
    "\n",
    "    # Extraction des liens depuis le PDF\n",
    "    if pdf_link and pdf_link != 'Lien PDF non disponible':\n",
    "        try:\n",
    "            print(f\"Ouverture du PDF pour l'article {article_id}: {pdf_link}\")\n",
    "            response = requests.get(pdf_link)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            with pdfplumber.open(io.BytesIO(response.content)) as pdf:\n",
    "                print(f\"Le PDF a été ouvert avec succès pour l'article {article_id}. Recherche de liens...\")\n",
    "                for page in pdf.pages:\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        links_from_pdf = extract_repository_urls(text)\n",
    "                        if links_from_pdf:\n",
    "                            software_links_set.update(links_from_pdf)\n",
    "                            print(f\"Liens trouvés dans le PDF pour l'article {article_id} : {links_from_pdf}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'ouverture du PDF pour l'article {article_id}: {e}\")\n",
    "\n",
    "    # Extraction des liens depuis l'abstract\n",
    "    if abstract and abstract != 'Abstract non disponible':\n",
    "        print(f\"Vérification de l'abstract pour l'article {article_id}.\")\n",
    "        links_from_abstract = extract_repository_urls(abstract)\n",
    "        if links_from_abstract:\n",
    "            software_links_set.update(links_from_abstract)\n",
    "            print(f\"Liens trouvés dans l'abstract pour l'article {article_id} : {links_from_abstract}\")\n",
    "\n",
    "    valid_links = [clean_link(link) for link in software_links_set if clean_link(link) and is_github_or_gitlab(link)]\n",
    "\n",
    "    # Mise à jour de la base de données selon la présence ou non de liens valides\n",
    "    if valid_links:\n",
    "        print(f\"Liens de dépôt logiciel valides pour l'article {article_id} : {valid_links}\")\n",
    "        c.execute(\"UPDATE articles SET software_links = ?, is_article_processed = 1 WHERE id = ?\", (', '.join(valid_links), article_id))\n",
    "    else:\n",
    "        print(f\"Aucun lien de dépôt logiciel trouvé pour l'article {article_id}.\")\n",
    "        c.execute(\"UPDATE articles SET is_article_processed = 2 WHERE id = ?\", (article_id,))\n",
    "\n",
    "    # Enregistrer les modifications dans la base de données\n",
    "    database.commit()\n",
    "    time.sleep(1)\n",
    "\n",
    "# Fonction pour traiter uniquement les nouveaux articles non traités (is_article_processed = 0)\n",
    "def process_new_articles():\n",
    "    c.execute(\"SELECT id, title, pdf_link, abstract FROM articles WHERE is_article_processed = 0\")\n",
    "    articles_to_process = c.fetchall()\n",
    "\n",
    "    # Vérifier s'il y a des articles à traiter\n",
    "    if not articles_to_process:\n",
    "        print(\"Aucun article à traiter.\")\n",
    "    else:\n",
    "        print(f\"{len(articles_to_process)} article(s) trouvé(s) à traiter.\")\n",
    "\n",
    "    for article in articles_to_process:\n",
    "        article_id, title, pdf_link, abstract = article\n",
    "        print(f\"Traitement de l'article : {title} (ID: {article_id})...\")\n",
    "        extract_software_links(article_id, pdf_link, abstract)\n",
    "\n",
    "# Appel de la fonction principale\n",
    "try:\n",
    "    process_new_articles()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Traitement interrompu. Les données sont sauvegardées.\")\n",
    "finally:\n",
    "    database.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliser ce code lorsqu'un article avec un temps de recherche dans le pdf trop long ,timer définit a 5min et passe après a l'abstract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Connexion à la base de données SQLite\n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor()\n",
    "\n",
    "# Vérifier et ajouter les colonnes software_links et is_article_processed si elles n'existent pas\n",
    "try:\n",
    "    c.execute(\"ALTER TABLE articles ADD COLUMN software_links TEXT\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass  # La colonne existe déjà\n",
    "\n",
    "try:\n",
    "    c.execute(\"ALTER TABLE articles ADD COLUMN is_article_processed INTEGER DEFAULT 0\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass  # La colonne existe déjà\n",
    "\n",
    "# Fonction pour vérifier si un lien est valide et s'il est un lien GitHub ou GitLab\n",
    "def is_valid_link(link):\n",
    "    return link.startswith('http://') or link.startswith('https://')\n",
    "\n",
    "def is_github_or_gitlab(link):\n",
    "    return 'github.com' in link or 'gitlab.com' in link\n",
    "\n",
    "# Fonction pour nettoyer les liens partiels ou incorrects\n",
    "def clean_link(link):\n",
    "    link = link.strip(').;,[]')  # Supprimer les caractères indésirables\n",
    "    if is_valid_link(link) and len(link) > len('https://github.com/'):\n",
    "        return link\n",
    "    return None\n",
    "\n",
    "# Fonction pour tester la validité des URLs via une requête HTTP\n",
    "def check_url_validity(link):\n",
    "    try:\n",
    "        response = requests.get(link, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Le lien est valide : {link}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Le lien a retourné un code de statut {response.status_code} pour : {link}\")\n",
    "            return False\n",
    "    except requests.RequestException:\n",
    "        print(f\"Erreur lors de la vérification du lien : {link}\")\n",
    "        return False\n",
    "\n",
    "# Fonction pour extraire les URLs des dépôts logiciels\n",
    "def extract_repository_urls(text):\n",
    "    url_pattern = r'(https?://[^\\s]+)'\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    repo_urls = []\n",
    "\n",
    "    for url in urls:\n",
    "        cleaned_url = clean_link(url)\n",
    "        if cleaned_url and is_github_or_gitlab(cleaned_url):\n",
    "            if check_url_validity(cleaned_url):  # Vérification de l'URL avant de l'ajouter\n",
    "                repo_urls.append(cleaned_url)\n",
    "\n",
    "    return repo_urls\n",
    "\n",
    "# Fonction pour extraire les liens de dépôt logiciel\n",
    "def extract_software_links(article_id, pdf_link, abstract):\n",
    "    software_links_set = set()\n",
    "\n",
    "    # Timer pour contrôler le temps d'attente\n",
    "    timer = None\n",
    "    \n",
    "    def time_up():\n",
    "        nonlocal timer\n",
    "        print(f\"Temps écoulé pour l'article {article_id}. Passage à l'abstract.\")\n",
    "        timer = None  # Annule le timer\n",
    "\n",
    "    # Extraction des liens depuis le PDF\n",
    "    if pdf_link and pdf_link != 'Lien PDF non disponible':\n",
    "        timer = Timer(300, time_up)  # 60 secondes = 1 minute\n",
    "        timer.start()\n",
    "\n",
    "        try:\n",
    "            print(f\"Ouverture du PDF pour l'article {article_id}: {pdf_link}\")\n",
    "            response = requests.get(pdf_link)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            with pdfplumber.open(io.BytesIO(response.content)) as pdf:\n",
    "                print(f\"Le PDF a été ouvert avec succès pour l'article {article_id}. Recherche de liens...\")\n",
    "                for page in pdf.pages:\n",
    "                    if timer is None:  # Vérifie si le timer est toujours actif\n",
    "                        text = page.extract_text()\n",
    "                        if text:\n",
    "                            links_from_pdf = extract_repository_urls(text)\n",
    "                            if links_from_pdf:\n",
    "                                software_links_set.update(links_from_pdf)\n",
    "                                print(f\"Liens trouvés dans le PDF pour l'article {article_id} : {links_from_pdf}\")\n",
    "                                break  # Sortir de la boucle si des liens sont trouvés\n",
    "                    else:\n",
    "                        print(\"Recherche dans le PDF abandonnée en raison du dépassement de temps.\")\n",
    "                        break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'ouverture du PDF pour l'article {article_id}: {e}\")\n",
    "\n",
    "        if timer is not None:\n",
    "            timer.cancel()  # Annuler le timer si l'extraction a réussi\n",
    "\n",
    "    # Extraction des liens depuis l'abstract\n",
    "    if not software_links_set and abstract and abstract != 'Abstract non disponible':\n",
    "        print(f\"Vérification de l'abstract pour l'article {article_id}.\")\n",
    "        links_from_abstract = extract_repository_urls(abstract)\n",
    "        if links_from_abstract:\n",
    "            software_links_set.update(links_from_abstract)\n",
    "            print(f\"Liens trouvés dans l'abstract pour l'article {article_id} : {links_from_abstract}\")\n",
    "\n",
    "    valid_links = [clean_link(link) for link in software_links_set if clean_link(link) and is_github_or_gitlab(link)]\n",
    "\n",
    "    # Mise à jour de la base de données selon la présence ou non de liens valides\n",
    "    if valid_links:\n",
    "        print(f\"Liens de dépôt logiciel valides pour l'article {article_id} : {valid_links}\")\n",
    "        c.execute(\"UPDATE articles SET software_links = ?, is_article_processed = 1 WHERE id = ?\", (', '.join(valid_links), article_id))\n",
    "    else:\n",
    "        print(f\"Aucun lien de dépôt logiciel trouvé pour l'article {article_id}.\")\n",
    "        c.execute(\"UPDATE articles SET is_article_processed = 2 WHERE id = ?\", (article_id,))\n",
    "\n",
    "    # Enregistrer les modifications dans la base de données\n",
    "    database.commit()\n",
    "    time.sleep(1)\n",
    "\n",
    "# Fonction pour traiter uniquement les nouveaux articles non traités (is_article_processed = 0)\n",
    "def process_new_articles():\n",
    "    c.execute(\"SELECT id, title, pdf_link, abstract FROM articles WHERE is_article_processed = 0\")\n",
    "    articles_to_process = c.fetchall()\n",
    "\n",
    "    # Vérifier s'il y a des articles à traiter\n",
    "    if not articles_to_process:\n",
    "        print(\"Aucun article à traiter.\")\n",
    "    else:\n",
    "        print(f\"{len(articles_to_process)} article(s) trouvé(s) à traiter.\")\n",
    "\n",
    "    for article in articles_to_process:\n",
    "        article_id, title, pdf_link, abstract = article\n",
    "        print(f\"Traitement de l'article : {title} (ID: {article_id})...\")\n",
    "        extract_software_links(article_id, pdf_link, abstract)\n",
    "\n",
    "# Appel de la fonction principale\n",
    "try:\n",
    "    process_new_articles()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Traitement interrompu. Les données sont sauvegardées.\")\n",
    "finally:\n",
    "    database.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.\tArchivage via Software Heritage:\n",
    "\n",
    "## Objectif\n",
    "Utiliser l’API de Software Heritage pour vérifier si le dépôt logiciel est déjà archivé dans leur base de données. Si le dépôt n’est pas encore archivé, soumettre automatiquement une demande d’archivage via l’API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Connexion à la base de données SQLite\n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor()\n",
    "\n",
    "# Ajouter la colonne 'archived' si elle n'existe pas déjà\n",
    "try:\n",
    "    c.execute(\"ALTER TABLE articles ADD COLUMN archived BOOLEAN DEFAULT FALSE;\")\n",
    "    print(\"Colonne 'archived' ajoutée avec succès.\")\n",
    "except sqlite3.OperationalError as e:\n",
    "    print(f\"La colonne 'archived' existe peut-être déjà : {e}\")\n",
    "\n",
    "\n",
    "# Token d'API Software Heritage\n",
    "TOKEN = \"eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJhMTMxYTQ1My1hM2IyLTQwMTUtODQ2Ny05MzAyZjk3MTFkOGEifQ.eyJpYXQiOjE3Mjk4NTA4NjUsImp0aSI6IjMyNGZhYzBkLWFjZmEtNDE3MS04ZmI1LTY2MzA2Y2FiYWVhZSIsImlzcyI6Imh0dHBzOi8vYXV0aC5zb2Z0d2FyZWhlcml0YWdlLm9yZy9hdXRoL3JlYWxtcy9Tb2Z0d2FyZUhlcml0YWdlIiwiYXVkIjoiaHR0cHM6Ly9hdXRoLnNvZnR3YXJlaGVyaXRhZ2Uub3JnL2F1dGgvcmVhbG1zL1NvZnR3YXJlSGVyaXRhZ2UiLCJzdWIiOiI0NTFmMTkzNy04ZTY4LTQxNjItYTk2Ny1lMjJkZjcwNDc5MmUiLCJ0eXAiOiJPZmZsaW5lIiwiYXpwIjoic3doLXdlYiIsInNlc3Npb25fc3RhdGUiOiJiYmQ4ZmExYS0yMDZkLTQzYzctYmQ3MS00MjMxMDE4MWYyNzMiLCJzY29wZSI6Im9wZW5pZCBvZmZsaW5lX2FjY2VzcyBwcm9maWxlIGVtYWlsIn0.cIPaxXVt65WiPz2FVb2y2ylQsyDJLUNQOSWIbe0hncE\"\n",
    "\n",
    "\n",
    "# Fonction pour vérifier si un dépôt est déjà archivé\n",
    "def check_archived(depot_url):\n",
    "    base_url = \"https://archive.softwareheritage.org/api/1/origin/\"\n",
    "    encoded_url = urllib.parse.quote(depot_url, safe='')  # Encodage URL pour éviter caractères spéciaux\n",
    "    check_url = f\"{base_url}{encoded_url}/get/\"\n",
    "    \n",
    "    logging.info(f\"Vérification de l'URL : {check_url}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(check_url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            logging.info(f\"Le dépôt {depot_url} est déjà archivé.\")\n",
    "            # Extraire le lien de l'archive et la date de la dernière archive\n",
    "            archive_info = response.json()\n",
    "\n",
    "            # Vérifie la structure de l'archive_info ici\n",
    "            logging.debug(f\"Informations d'archive: {archive_info}\")  # Ajoute un log pour inspecter les données\n",
    "\n",
    "            # Utilise les bonnes clés selon la réponse de l'API\n",
    "            if 'archive' in archive_info:\n",
    "                archive_link = archive_info['archive'].get('url', 'Lien non disponible')\n",
    "                last_archive_date = archive_info['archive'].get('date', 'Date non disponible')\n",
    "            else:\n",
    "                archive_link = 'Lien non disponible'\n",
    "                last_archive_date = 'Date non disponible'\n",
    "\n",
    "            logging.info(f\"Lien de l'archive : {archive_link}, Date de la dernière archive : {last_archive_date}\")\n",
    "            return True, archive_link, last_archive_date\n",
    "        \n",
    "        elif response.status_code == 404:\n",
    "            logging.info(f\"Le dépôt {depot_url} n'est pas encore archivé.\")\n",
    "            return False, None, None\n",
    "            \n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Erreur lors de la vérification de {depot_url} : {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "# Fonction pour archiver le dépôt\n",
    "def archive_repo(depot_url,c):\n",
    "    # Type du dépôt, ici \"git\"\n",
    "    visit_type = \"git\"\n",
    "    \n",
    "    # Construire l'URL pour l'archivage\n",
    "    encoded_url = urllib.parse.quote(depot_url, safe='')  # Encodage URL\n",
    "    save_url = f\"https://archive.softwareheritage.org/api/1/origin/save/{visit_type}/url/{encoded_url}/\"\n",
    "    \n",
    "    logging.info(f\"Tentative d'archivage pour : {save_url}\")\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {TOKEN}' \n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(save_url, headers=headers)\n",
    "        \n",
    "        # Gérer les erreurs de manière spécifique\n",
    "        if response.status_code == 200:\n",
    "            logging.info(f\"Le dépôt {depot_url} a été soumis pour archivage avec succès.\")\n",
    "            # Mettre à jour la base de données pour marquer le dépôt comme archivé\n",
    "            c.execute(\"UPDATE articles SET archived = TRUE WHERE software_links LIKE ?\", ('%' + depot_url + '%',))\n",
    "            database.commit()  # Commit des changements\n",
    "        elif response.status_code == 301:\n",
    "            logging.warning(f\"Erreur 301 : Redirection détectée pour {depot_url}. Vérifiez l'URL.\")\n",
    "        elif response.status_code == 403:\n",
    "            logging.warning(\"Erreur 403 (Forbidden) : Le token est peut-être invalide ou a expiré.\")\n",
    "        elif response.status_code == 404:\n",
    "            logging.warning(f\"Erreur 404 : L'URL de {depot_url} n'existe pas.\")\n",
    "        elif response.status_code == 429:\n",
    "            logging.warning(\"Erreur 429 (Trop de requêtes). Pause de 60 secondes.\")\n",
    "            time.sleep(60)\n",
    "            archive_repo(depot_url)  # Relancer après une pause\n",
    "        else:\n",
    "            logging.warning(f\"Erreur lors de la soumission de {depot_url} : {response.status_code} - {response.text}\")\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Erreur lors de la soumission de {depot_url} : {e}\")\n",
    "        time.sleep(5)\n",
    "        archive_repo(depot_url)  # Réessayer après un délai en cas d'erreur temporaire\n",
    "\n",
    "\n",
    "\n",
    "# Limites de requêtes\n",
    "MAX_REQUESTS_PER_HOUR = 1200\n",
    "TIME_BETWEEN_REQUESTS = 3600 / MAX_REQUESTS_PER_HOUR\n",
    "\n",
    "# Lire les URL des dépôts dans la base de données\n",
    "def process_repositories():\n",
    "    c.execute(\"SELECT software_links FROM articles WHERE software_links IS NOT NULL\")\n",
    "    software_links = c.fetchall()\n",
    "\n",
    "    for link_tuple in software_links:\n",
    "        depot_urls = link_tuple[0].split(',')\n",
    "\n",
    "        for depot_url in depot_urls:\n",
    "            depot_url = depot_url.strip()\n",
    "\n",
    "            if depot_url and isinstance(depot_url, str) and depot_url.startswith(\"http\"):\n",
    "                \n",
    "                # Vérifier si le dépôt est déjà archivé\n",
    "                c.execute(\"SELECT archived FROM articles WHERE software_links LIKE ?\", ('%' + depot_url + '%',))\n",
    "                archived = c.fetchone()\n",
    "                \n",
    "                if archived and archived[0]:  # Si déjà archivé, passer au suivant\n",
    "                    logging.info(f\"Le dépôt {depot_url} est déjà archivé, saut de l'archivage.\")\n",
    "                    continue  # Passer à l'URL suivante\n",
    "                \n",
    "                # Vérifier si le dépôt est déjà archivé et récupérer les informations d'archive\n",
    "                is_archived, archive_link, last_archive_date = check_archived(depot_url)\n",
    "                \n",
    "                if not is_archived:\n",
    "                    archive_repo(depot_url,c)\n",
    "                    # Une fois archivé, mettre à jour la base de données\n",
    "                    c.execute(\"UPDATE articles SET archived = TRUE WHERE software_links LIKE ?\", ('%' + depot_url + '%',))\n",
    "                    database.commit()  # Commit les changements\n",
    "                    time.sleep(TIME_BETWEEN_REQUESTS)  # Pause entre les requêtes\n",
    "                else:\n",
    "                    # Si le dépôt est déjà archivé, tu peux décider de faire quelque chose avec ces informations\n",
    "                    logging.info(f\"Dépôt déjà archivé : {archive_link} à la date {last_archive_date}\")\n",
    "                    # Peut-être stocker ces informations dans la base de données si nécessaire\n",
    "                \n",
    "            else:\n",
    "                logging.warning(f\"URL invalide trouvée : {depot_url}\")\n",
    "\n",
    "\n",
    "            \n",
    "# Exécuter la fonction principale\n",
    "process_repositories()\n",
    "\n",
    "# Fermer la connexion à la base de données\n",
    "database.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR ME : check la table sur le jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connexion à la base de données SQLite\n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor()\n",
    "\n",
    "# Fonction pour afficher les attributs de la table articles\n",
    "def display_articles():\n",
    "    # Exécuter une requête pour récupérer toutes les colonnes et enregistrements de la table articles\n",
    "    c.execute(\"SELECT * FROM articles\")\n",
    "    \n",
    "    # Récupérer les noms de colonnes\n",
    "    column_names = [description[0] for description in c.description]\n",
    "    \n",
    "    # Afficher les noms des colonnes\n",
    "    print(f\"{' | '.join(column_names)}\")\n",
    "    print(\"-\" * (len(column_names) * 4))  # Ligne de séparation\n",
    "    \n",
    "    # Récupérer et afficher tous les enregistrements\n",
    "    for row in c.fetchall():\n",
    "        print(' | '.join(map(str, row)))\n",
    "\n",
    "# Appeler la fonction pour afficher les articles\n",
    "display_articles()\n",
    "\n",
    "# Fermer la connexion à la base de données\n",
    "database.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le faire en cas de soucis and need tout recommencer \n",
    "#c.execute(\"DROP TABLE IF EXISTS articles\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
