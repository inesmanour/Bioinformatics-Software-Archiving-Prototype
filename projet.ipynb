{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation de bibliotheques \n",
    "\n",
    "**pip install beautifulsoup4**\n",
    "\n",
    "**pip install requests**\n",
    "\n",
    "**pip install PyPDF2**\n",
    "\n",
    "**pip install pdfplumber**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pdfplumber\n",
    "import re, io\n",
    "import PyPDF2\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "import time\n",
    "from threading import Timer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraper les articles bioRxiv liés à la bioinformatique "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parcourir les articles de la collection Bioinformatics , scraper 10 pages à chaque exécution . (run toutes les semaines puisque new articles se mette en page 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'https://www.biorxiv.org'\n",
    "\n",
    "# Créer la base de données SQLite \"bioinformatics_articles\"\n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor()\n",
    "\n",
    "# Créer la table 'articles' si elle n'existe pas\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS articles\n",
    "             (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "             title TEXT, \n",
    "             link TEXT,\n",
    "             doi TEXT UNIQUE,\n",
    "             date TEXT,\n",
    "             pdf_link TEXT,\n",
    "             abstract TEXT)''')\n",
    "\n",
    "# Nombre de pages à scraper (exemple : 10 pages à la fois)\n",
    "total_pages = 10\n",
    "\n",
    "# Boucle qui parcourt chaque page de la collection Bioinformatics\n",
    "for page in range(1, total_pages + 1):\n",
    "    print(f\"Scraping page {page} sur {total_pages}\")\n",
    "\n",
    "    # Requête qui récupère la page bioinfo actuelle\n",
    "    response = requests.get(f'{url}/collection/bioinformatics?page={page}')\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Trouver les liens vers les articles sur cette page\n",
    "    articles = soup.find_all('a', class_='highwire-cite-linked-title')\n",
    "\n",
    "    print(f'Nombre total d\\'articles trouvés sur la page {page} : {len(articles)}')\n",
    "\n",
    "    # Parcourir tous les articles de la page et enregistrer leurs titres et liens\n",
    "    for article in articles:\n",
    "        title = article.text.strip()\n",
    "        link = url + article['href']\n",
    "\n",
    "        # Requête pour accéder à la page de l'article\n",
    "        article_response = requests.get(link)\n",
    "        article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "\n",
    "        # Extraire le DOI\n",
    "        doi_tag = article_soup.find('meta', {'name': 'citation_doi'})\n",
    "        doi = doi_tag['content'] if doi_tag else 'DOI non disponible'\n",
    "\n",
    "        # Extraire la date de publication\n",
    "        date_tag = article_soup.find('meta', {'name': 'citation_publication_date'})\n",
    "        date = date_tag['content'] if date_tag else 'Date non disponible'\n",
    "        if date != 'Date non disponible':\n",
    "            date = date.replace('/', '-')  # Remplacer '/' par '-'\n",
    "            date = datetime.strptime(date, '%Y-%m-%d').date().isoformat()\n",
    "\n",
    "        # Extraire le lien vers le fichier PDF\n",
    "        pdf_links = article_soup.find_all('a')\n",
    "        pdf_link = next((url + pdf['href'] for pdf in pdf_links if 'PDF' in pdf.text and pdf['href'].endswith('.pdf')), 'Lien PDF non disponible')\n",
    "\n",
    "        # Extraire l'abstract\n",
    "        abstract_tag = article_soup.find('meta', {'name': 'citation_abstract'})\n",
    "        clean_abstract = abstract_tag['content'] if abstract_tag else 'Abstract non disponible'\n",
    "\n",
    "        # Supprimer les balises <p> au début et à la fin de l'abstract\n",
    "        if clean_abstract.startswith('<p>'):\n",
    "            clean_abstract = clean_abstract[3:]  # Supprimer les 3 premiers caractères\n",
    "        if clean_abstract.endswith('</p>'):\n",
    "            clean_abstract = clean_abstract[:-4]  # Supprimer les 4 derniers caractères\n",
    "\n",
    "        # Insérer dans la base de données SQLite\n",
    "        c.execute(\"SELECT * FROM articles WHERE doi = ? AND date = ?\", (doi, date))\n",
    "        result = c.fetchone()\n",
    "\n",
    "        if result:\n",
    "            print(f\"L'article avec DOI {doi} existe déjà, ignoré.\")\n",
    "        else:\n",
    "            c.execute(\"INSERT INTO articles (title, link, doi, date, pdf_link, abstract) VALUES (?, ?, ?, ?, ?, ?)\", \n",
    "                      (title, link, doi, date, pdf_link, clean_abstract))\n",
    "            print(f\"Article ajouté : {title}\")\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Sauvegarder les modifications dans la base de données après chaque page\n",
    "    database.commit()\n",
    "    \n",
    "database.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extraction des URLs des Dépôts Logiciels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1.\tVérification et ajout des colonnes nécessaires dans la base de données.\n",
    "\n",
    "2.\tValidation des liens pour s’assurer qu’ils sont valides et pertinents.\n",
    "\n",
    "3.\tVérification de l’accessibilité des URLs via des requêtes HTTP.\n",
    "\n",
    "4.\tExtraction des URLs des dépôts à partir des contenus des PDF et des abstracts.\n",
    "\n",
    "5.\tTraitement des nouveaux articles non encore traités pour l’extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les articles qui ont été vérifiés, mais sans trouver de liens logiciels, seront mis à jour avec is_article_processed = 2. Cela permet de ne pas les vérifier à nouveau lors des exécutions futures.---> Le script ne traite que les articles où is_article_processed = 0. Cela évite de revérifier les articles déjà traités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "import re\n",
    "import pdfplumber\n",
    "import io\n",
    "import time\n",
    "\n",
    "# Connexion à la base de données SQLite\n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor()\n",
    "\n",
    "# Vérifier et ajouter les colonnes software_links et is_article_processed si elles n'existent pas\n",
    "try:\n",
    "    c.execute(\"ALTER TABLE articles ADD COLUMN software_links TEXT\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass  # La colonne existe déjà\n",
    "\n",
    "try:\n",
    "    c.execute(\"ALTER TABLE articles ADD COLUMN is_article_processed INTEGER DEFAULT 0\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass  # La colonne existe déjà\n",
    "\n",
    "# Fonction pour vérifier si un lien est valide et s'il est un lien GitHub ou GitLab\n",
    "def is_valid_link(link):\n",
    "    return link.startswith('http://') or link.startswith('https://')\n",
    "\n",
    "def is_github_or_gitlab(link):\n",
    "    return 'github.com' in link or 'gitlab.com' in link\n",
    "\n",
    "# Fonction pour nettoyer les liens partiels ou incorrects\n",
    "def clean_link(link):\n",
    "    link = link.strip(').;,[]')  # Supprimer les caractères indésirables\n",
    "    if is_valid_link(link) and len(link) > len('https://github.com/'):\n",
    "        return link\n",
    "    return None\n",
    "\n",
    "# Fonction pour tester la validité des URLs via une requête HTTP\n",
    "def check_url_validity(link):\n",
    "    try:\n",
    "        response = requests.get(link, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Le lien est valide : {link}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Le lien a retourné un code de statut {response.status_code} pour : {link}\")\n",
    "            return False\n",
    "    except requests.RequestException:\n",
    "        print(f\"Erreur lors de la vérification du lien : {link}\")\n",
    "        return False\n",
    "\n",
    "# Fonction pour extraire les URLs des dépôts logiciels\n",
    "def extract_repository_urls(text):\n",
    "    url_pattern = r'(https?://[^\\s]+)'\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    repo_urls = []\n",
    "\n",
    "    for url in urls:\n",
    "        cleaned_url = clean_link(url)\n",
    "        if cleaned_url and is_github_or_gitlab(cleaned_url):\n",
    "            if check_url_validity(cleaned_url):  # Vérification de l'URL avant de l'ajouter\n",
    "                repo_urls.append(cleaned_url)\n",
    "\n",
    "    return repo_urls\n",
    "\n",
    "# Fonction pour extraire les liens de dépôt logiciel\n",
    "def extract_software_links(article_id, pdf_link, abstract):\n",
    "    software_links_set = set()\n",
    "\n",
    "    # Extraction des liens depuis le PDF\n",
    "    if pdf_link and pdf_link != 'Lien PDF non disponible':\n",
    "        try:\n",
    "            print(f\"Ouverture du PDF pour l'article {article_id}: {pdf_link}\")\n",
    "            response = requests.get(pdf_link)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            with pdfplumber.open(io.BytesIO(response.content)) as pdf:\n",
    "                print(f\"Le PDF a été ouvert avec succès pour l'article {article_id}. Recherche de liens...\")\n",
    "                for page in pdf.pages:\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        links_from_pdf = extract_repository_urls(text)\n",
    "                        if links_from_pdf:\n",
    "                            software_links_set.update(links_from_pdf)\n",
    "                            print(f\"Liens trouvés dans le PDF pour l'article {article_id} : {links_from_pdf}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'ouverture du PDF pour l'article {article_id}: {e}\")\n",
    "\n",
    "    # Extraction des liens depuis l'abstract\n",
    "    if abstract and abstract != 'Abstract non disponible':\n",
    "        print(f\"Vérification de l'abstract pour l'article {article_id}.\")\n",
    "        links_from_abstract = extract_repository_urls(abstract)\n",
    "        if links_from_abstract:\n",
    "            software_links_set.update(links_from_abstract)\n",
    "            print(f\"Liens trouvés dans l'abstract pour l'article {article_id} : {links_from_abstract}\")\n",
    "\n",
    "    valid_links = [clean_link(link) for link in software_links_set if clean_link(link) and is_github_or_gitlab(link)]\n",
    "\n",
    "    # Mise à jour de la base de données selon la présence ou non de liens valides\n",
    "    if valid_links:\n",
    "        print(f\"Liens de dépôt logiciel valides pour l'article {article_id} : {valid_links}\")\n",
    "        c.execute(\"UPDATE articles SET software_links = ?, is_article_processed = 1 WHERE id = ?\", (', '.join(valid_links), article_id))\n",
    "    else:\n",
    "        print(f\"Aucun lien de dépôt logiciel trouvé pour l'article {article_id}.\")\n",
    "        c.execute(\"UPDATE articles SET is_article_processed = 2 WHERE id = ?\", (article_id,))\n",
    "\n",
    "    # Enregistrer les modifications dans la base de données\n",
    "    database.commit()\n",
    "    time.sleep(1)\n",
    "\n",
    "# Fonction pour traiter uniquement les nouveaux articles non traités (is_article_processed = 0)\n",
    "def process_new_articles():\n",
    "    c.execute(\"SELECT id, title, pdf_link, abstract FROM articles WHERE is_article_processed = 0\")\n",
    "    articles_to_process = c.fetchall()\n",
    "\n",
    "    # Vérifier s'il y a des articles à traiter\n",
    "    if not articles_to_process:\n",
    "        print(\"Aucun article à traiter.\")\n",
    "    else:\n",
    "        print(f\"{len(articles_to_process)} article(s) trouvé(s) à traiter.\")\n",
    "\n",
    "    for article in articles_to_process:\n",
    "        article_id, title, pdf_link, abstract = article\n",
    "        print(f\"Traitement de l'article : {title} (ID: {article_id})...\")\n",
    "        extract_software_links(article_id, pdf_link, abstract)\n",
    "\n",
    "# Appel de la fonction principale\n",
    "try:\n",
    "    process_new_articles()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Traitement interrompu. Les données sont sauvegardées.\")\n",
    "finally:\n",
    "    database.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliser ce code lorsqu'un article avec un temps de recherche dans le pdf trop long ,timer définit a 5min et passe après a l'abstract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "import re\n",
    "import pdfplumber\n",
    "import io\n",
    "import time\n",
    "from threading import Timer\n",
    "\n",
    "# Connexion à la base de données SQLite\n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor()\n",
    "\n",
    "# Vérifier et ajouter les colonnes software_links et is_article_processed si elles n'existent pas\n",
    "try:\n",
    "    c.execute(\"ALTER TABLE articles ADD COLUMN software_links TEXT\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass  # La colonne existe déjà\n",
    "\n",
    "try:\n",
    "    c.execute(\"ALTER TABLE articles ADD COLUMN is_article_processed INTEGER DEFAULT 0\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass  # La colonne existe déjà\n",
    "\n",
    "# Fonction pour vérifier si un lien est valide et s'il est un lien GitHub ou GitLab\n",
    "def is_valid_link(link):\n",
    "    return link.startswith('http://') or link.startswith('https://')\n",
    "\n",
    "def is_github_or_gitlab(link):\n",
    "    return 'github.com' in link or 'gitlab.com' in link\n",
    "\n",
    "# Fonction pour nettoyer les liens partiels ou incorrects\n",
    "def clean_link(link):\n",
    "    link = link.strip(').;,[]')  # Supprimer les caractères indésirables\n",
    "    if is_valid_link(link) and len(link) > len('https://github.com/'):\n",
    "        return link\n",
    "    return None\n",
    "\n",
    "# Fonction pour tester la validité des URLs via une requête HTTP\n",
    "def check_url_validity(link):\n",
    "    try:\n",
    "        response = requests.get(link, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Le lien est valide : {link}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Le lien a retourné un code de statut {response.status_code} pour : {link}\")\n",
    "            return False\n",
    "    except requests.RequestException:\n",
    "        print(f\"Erreur lors de la vérification du lien : {link}\")\n",
    "        return False\n",
    "\n",
    "# Fonction pour extraire les URLs des dépôts logiciels\n",
    "def extract_repository_urls(text):\n",
    "    url_pattern = r'(https?://[^\\s]+)'\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    repo_urls = []\n",
    "\n",
    "    for url in urls:\n",
    "        cleaned_url = clean_link(url)\n",
    "        if cleaned_url and is_github_or_gitlab(cleaned_url):\n",
    "            if check_url_validity(cleaned_url):  # Vérification de l'URL avant de l'ajouter\n",
    "                repo_urls.append(cleaned_url)\n",
    "\n",
    "    return repo_urls\n",
    "\n",
    "# Fonction pour extraire les liens de dépôt logiciel\n",
    "def extract_software_links(article_id, pdf_link, abstract):\n",
    "    software_links_set = set()\n",
    "\n",
    "    # Timer pour contrôler le temps d'attente\n",
    "    timer = None\n",
    "    \n",
    "    def time_up():\n",
    "        nonlocal timer\n",
    "        print(f\"Temps écoulé pour l'article {article_id}. Passage à l'abstract.\")\n",
    "        timer = None  # Annule le timer\n",
    "\n",
    "    # Extraction des liens depuis le PDF\n",
    "    if pdf_link and pdf_link != 'Lien PDF non disponible':\n",
    "        timer = Timer(300, time_up)  # 60 secondes = 1 minute\n",
    "        timer.start()\n",
    "\n",
    "        try:\n",
    "            print(f\"Ouverture du PDF pour l'article {article_id}: {pdf_link}\")\n",
    "            response = requests.get(pdf_link)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            with pdfplumber.open(io.BytesIO(response.content)) as pdf:\n",
    "                print(f\"Le PDF a été ouvert avec succès pour l'article {article_id}. Recherche de liens...\")\n",
    "                for page in pdf.pages:\n",
    "                    if timer is None:  # Vérifie si le timer est toujours actif\n",
    "                        text = page.extract_text()\n",
    "                        if text:\n",
    "                            links_from_pdf = extract_repository_urls(text)\n",
    "                            if links_from_pdf:\n",
    "                                software_links_set.update(links_from_pdf)\n",
    "                                print(f\"Liens trouvés dans le PDF pour l'article {article_id} : {links_from_pdf}\")\n",
    "                                break  # Sortir de la boucle si des liens sont trouvés\n",
    "                    else:\n",
    "                        print(\"Recherche dans le PDF abandonnée en raison du dépassement de temps.\")\n",
    "                        break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'ouverture du PDF pour l'article {article_id}: {e}\")\n",
    "\n",
    "        if timer is not None:\n",
    "            timer.cancel()  # Annuler le timer si l'extraction a réussi\n",
    "\n",
    "    # Extraction des liens depuis l'abstract\n",
    "    if not software_links_set and abstract and abstract != 'Abstract non disponible':\n",
    "        print(f\"Vérification de l'abstract pour l'article {article_id}.\")\n",
    "        links_from_abstract = extract_repository_urls(abstract)\n",
    "        if links_from_abstract:\n",
    "            software_links_set.update(links_from_abstract)\n",
    "            print(f\"Liens trouvés dans l'abstract pour l'article {article_id} : {links_from_abstract}\")\n",
    "\n",
    "    valid_links = [clean_link(link) for link in software_links_set if clean_link(link) and is_github_or_gitlab(link)]\n",
    "\n",
    "    # Mise à jour de la base de données selon la présence ou non de liens valides\n",
    "    if valid_links:\n",
    "        print(f\"Liens de dépôt logiciel valides pour l'article {article_id} : {valid_links}\")\n",
    "        c.execute(\"UPDATE articles SET software_links = ?, is_article_processed = 1 WHERE id = ?\", (', '.join(valid_links), article_id))\n",
    "    else:\n",
    "        print(f\"Aucun lien de dépôt logiciel trouvé pour l'article {article_id}.\")\n",
    "        c.execute(\"UPDATE articles SET is_article_processed = 2 WHERE id = ?\", (article_id,))\n",
    "\n",
    "    # Enregistrer les modifications dans la base de données\n",
    "    database.commit()\n",
    "    time.sleep(1)\n",
    "\n",
    "# Fonction pour traiter uniquement les nouveaux articles non traités (is_article_processed = 0)\n",
    "def process_new_articles():\n",
    "    c.execute(\"SELECT id, title, pdf_link, abstract FROM articles WHERE is_article_processed = 0\")\n",
    "    articles_to_process = c.fetchall()\n",
    "\n",
    "    # Vérifier s'il y a des articles à traiter\n",
    "    if not articles_to_process:\n",
    "        print(\"Aucun article à traiter.\")\n",
    "    else:\n",
    "        print(f\"{len(articles_to_process)} article(s) trouvé(s) à traiter.\")\n",
    "\n",
    "    for article in articles_to_process:\n",
    "        article_id, title, pdf_link, abstract = article\n",
    "        print(f\"Traitement de l'article : {title} (ID: {article_id})...\")\n",
    "        extract_software_links(article_id, pdf_link, abstract)\n",
    "\n",
    "# Appel de la fonction principale\n",
    "try:\n",
    "    process_new_articles()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Traitement interrompu. Les données sont sauvegardées.\")\n",
    "finally:\n",
    "    database.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  FOR ME --> SUPPRIMER PLUS TARD : Affichage de mes articles dans la base de données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le faire en cas de soucis and need tout recommencer \n",
    "#c.execute(\"DROP TABLE IF EXISTS articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connexion à la base de données SQLite\n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor()\n",
    "\n",
    "# Fonction pour afficher les attributs de la table articles\n",
    "def display_articles():\n",
    "    # Exécuter une requête pour récupérer toutes les colonnes et enregistrements de la table articles\n",
    "    c.execute(\"SELECT * FROM articles\")\n",
    "    \n",
    "    # Récupérer les noms de colonnes\n",
    "    column_names = [description[0] for description in c.description]\n",
    "    \n",
    "    # Afficher les noms des colonnes\n",
    "    print(f\"{' | '.join(column_names)}\")\n",
    "    print(\"-\" * (len(column_names) * 4))  # Ligne de séparation\n",
    "    \n",
    "    # Récupérer et afficher tous les enregistrements\n",
    "    for row in c.fetchall():\n",
    "        print(' | '.join(map(str, row)))\n",
    "\n",
    "# Appeler la fonction pour afficher les articles\n",
    "display_articles()\n",
    "\n",
    "# Fermer la connexion à la base de données\n",
    "database.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# suite du projet  :\n",
    "\n",
    "3.\tArchivage via Software Heritage:\n",
    "\n",
    "Utiliser l’API de Software Heritage pour vérifier si le dépôt logiciel est déjà archivé dans leur base de données. Si le dépôt n’est pas encore archivé, soumettre automatiquement une demande d’archivage via l’API.\n",
    "\n",
    "--->Script 3 : lire la BDD, récupérer les URL, vérifier que le dépôt est archivé dans Software Heritage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le dépôt https://github.com/Cai-Lab-at-University-of-Michigan/pySISF n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/Cai-Lab-at-University-of-Michigan/pySISF: 404\n",
      "Le dépôt https://github.com/Cai-Lab-at-University-of-Michigan/SISF_CDN n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/Cai-Lab-at-University-of-Michigan/SISF_CDN: 404\n",
      "Le dépôt https://github.com/Cai-Lab-at-University-of-Michigan/nTracer2 n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/Cai-Lab-at-University-of-Michigan/nTracer2: 404\n",
      "Le dépôt https://github.com/camlab-bioml/genbait_reproducibility n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/camlab-bioml/genbait_reproducibility: 404\n",
      "Le dépôt https://github.com/camlab-bioml/genbait n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/camlab-bioml/genbait: 404\n",
      "Le dépôt https://github.com/Exscientia/ab-characterisation n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/Exscientia/ab-characterisation: 404\n",
      "Le dépôt https://github.com/avelar-ageing/senescence_stress n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/avelar-ageing/senescence_stress: 404\n",
      "Le dépôt https://github.com/rstudio/shiny n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/rstudio/shiny: 404\n",
      "Le dépôt https://github.com/PacificBiosciences/pbsv n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/PacificBiosciences/pbsv: 404\n",
      "Le dépôt https://github.com/e-lerat/replicaTE n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/e-lerat/replicaTE: 404\n",
      "Le dépôt https://github.com/tseemann/barrnap n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/tseemann/barrnap: 404\n",
      "Le dépôt https://github.com/Sulam-Group/BiGraph4TME.git n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/Sulam-Group/BiGraph4TME.git: 404\n",
      "Le dépôt https://github.com/Lab-CoMBINE/PoreMeth2 n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/Lab-CoMBINE/PoreMeth2: 404\n",
      "Le dépôt https://github.com/deemeng/punch2 n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/deemeng/punch2: 404\n",
      "Le dépôt https://github.com/deemeng/punch2_light n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/deemeng/punch2_light: 404\n",
      "Le dépôt https://github.com/KorkinLab/infection-dynamics n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/KorkinLab/infection-dynamics: 404\n",
      "Le dépôt https://github.com/TransDecoder/TransDecoder n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/TransDecoder/TransDecoder: 404\n",
      "Le dépôt https://github.com/shergreen/pyppca n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/shergreen/pyppca: 404\n",
      "Le dépôt https://github.com/padilr1/ChIPbinner n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/padilr1/ChIPbinner: 404\n",
      "Le dépôt https://github.com/padilr1/ChIPbinner_database n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/padilr1/ChIPbinner_database: 404\n",
      "Le dépôt https://github.com/PeterZZQ/scMoMaT/blob/main/demo_scmomat.ipynb n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/PeterZZQ/scMoMaT/blob/main/demo_scmomat.ipynb: 404\n",
      "Le dépôt https://github.com/MarioniLab/StabMap/tree/main/vignettes n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/MarioniLab/StabMap/tree/main/vignettes: 404\n",
      "Le dépôt https://github.com/epurdom/cobolt/blob/master/docs/tutorial.ipynb n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/epurdom/cobolt/blob/master/docs/tutorial.ipynb: 404\n",
      "Le dépôt https://github.com/JinmiaoChenLab/SpaMosaic n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/JinmiaoChenLab/SpaMosaic: 404\n",
      "Le dépôt https://github.com/spotify/annoy n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/spotify/annoy: 404\n",
      "Le dépôt https://github.com/ncbi/SilencerEnhancerPredict n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/ncbi/SilencerEnhancerPredict: 404\n",
      "Le dépôt https://github.com/lyli1013/DeepICSH n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/lyli1013/DeepICSH: 404\n",
      "Le dépôt https://github.com/cuixj19/CREATE n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/cuixj19/CREATE: 404\n",
      "Le dépôt https://github.com/uci-cbcl/DanQ n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/uci-cbcl/DanQ: 404\n",
      "Le dépôt https://github.com/hasindu2008/slow5tools n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/hasindu2008/slow5tools: 404\n",
      "Le dépôt https://github.com/nanoporetech/vbz_compression n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/nanoporetech/vbz_compression: 404\n",
      "Le dépôt https://github.com/hasindu2008/slow5lib n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/hasindu2008/slow5lib: 404\n",
      "Le dépôt https://github.com/nanoporetech/pod5-file-format n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/nanoporetech/pod5-file-format: 404\n",
      "Le dépôt https://github.com/WoldringLabMSU/EvoSeq-ML n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/WoldringLabMSU/EvoSeq-ML: 404\n",
      "Le dépôt https://github.com/aaronwtr/PertEval n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/aaronwtr/PertEval: 404\n",
      "Le dépôt https://github.com/gefeiwang/scMODAL n'est pas encore archivé.\n",
      "Erreur lors de la soumission pour https://github.com/gefeiwang/scMODAL: 404\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m url_list:\n\u001b[1;32m     41\u001b[0m         url \u001b[38;5;241m=\u001b[39m url\u001b[38;5;241m.\u001b[39mstrip()  \u001b[38;5;66;03m# Retirer les espaces en trop\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcheck_archived\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     43\u001b[0m             submit_for_archiving(url)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Fermer la connexion à la base de données\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[42], line 5\u001b[0m, in \u001b[0;36mcheck_archived\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      3\u001b[0m api_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.softwareheritage.org/api/1/origin/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLe dépôt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m est déjà archivé.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/requests/api.py:76\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/requests/api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/requests/sessions.py:532\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(req)\n\u001b[1;32m    530\u001b[0m proxies \u001b[38;5;241m=\u001b[39m proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 532\u001b[0m settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_environment_settings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcert\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    537\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m: timeout,\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m'\u001b[39m: allow_redirects,\n\u001b[1;32m    540\u001b[0m }\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/requests/sessions.py:711\u001b[0m, in \u001b[0;36mSession.merge_environment_settings\u001b[0;34m(self, url, proxies, stream, verify, cert)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrust_env:\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;66;03m# Set environment's proxies.\u001b[39;00m\n\u001b[1;32m    710\u001b[0m     no_proxy \u001b[38;5;241m=\u001b[39m proxies\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_proxy\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m proxies \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 711\u001b[0m     env_proxies \u001b[38;5;241m=\u001b[39m \u001b[43mget_environ_proxies\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_proxy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (k, v) \u001b[38;5;129;01min\u001b[39;00m env_proxies\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    713\u001b[0m         proxies\u001b[38;5;241m.\u001b[39msetdefault(k, v)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/requests/utils.py:776\u001b[0m, in \u001b[0;36mget_environ_proxies\u001b[0;34m(url, no_proxy)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_environ_proxies\u001b[39m(url, no_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;124;03m    Return a dict of environment proxies.\u001b[39;00m\n\u001b[1;32m    773\u001b[0m \n\u001b[1;32m    774\u001b[0m \u001b[38;5;124;03m    :rtype: dict\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 776\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mshould_bypass_proxies\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_proxy\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    777\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/requests/utils.py:760\u001b[0m, in \u001b[0;36mshould_bypass_proxies\u001b[0;34m(url, no_proxy)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_environ(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_proxy\u001b[39m\u001b[38;5;124m'\u001b[39m, no_proxy_arg):\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;66;03m# parsed.hostname can be `None` in cases such as a file URI.\u001b[39;00m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 760\u001b[0m         bypass \u001b[38;5;241m=\u001b[39m \u001b[43mproxy_bypass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, socket\u001b[38;5;241m.\u001b[39mgaierror):\n\u001b[1;32m    762\u001b[0m         bypass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/urllib/request.py:2647\u001b[0m, in \u001b[0;36mproxy_bypass\u001b[0;34m(host)\u001b[0m\n\u001b[1;32m   2645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m proxy_bypass_environment(host, proxies)\n\u001b[1;32m   2646\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mproxy_bypass_macosx_sysconf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/urllib/request.py:2624\u001b[0m, in \u001b[0;36mproxy_bypass_macosx_sysconf\u001b[0;34m(host)\u001b[0m\n\u001b[1;32m   2622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproxy_bypass_macosx_sysconf\u001b[39m(host):\n\u001b[1;32m   2623\u001b[0m     proxy_settings \u001b[38;5;241m=\u001b[39m _get_proxy_settings()\n\u001b[0;32m-> 2624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_proxy_bypass_macosx_sysconf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxy_settings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/urllib/request.py:2566\u001b[0m, in \u001b[0;36m_proxy_bypass_macosx_sysconf\u001b[0;34m(host, proxy_settings)\u001b[0m\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_proxy_bypass_macosx_sysconf\u001b[39m(host, proxy_settings):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m \u001b[38;5;124;03m    Return True iff this host shouldn't be accessed using a proxy\u001b[39;00m\n\u001b[1;32m   2557\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2564\u001b[0m \u001b[38;5;124;03m    }\u001b[39;00m\n\u001b[1;32m   2565\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2566\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfnmatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fnmatch\n\u001b[1;32m   2568\u001b[0m     hostonly, port \u001b[38;5;241m=\u001b[39m _splitport(host)\n\u001b[1;32m   2570\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mip2num\u001b[39m(ipAddr):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Fonction pour vérifier si un dépôt est déjà archivé sur Software Heritage\n",
    "def check_archived(url):\n",
    "    api_url = f\"https://archive.softwareheritage.org/api/1/origin/{url}/\"\n",
    "    try:\n",
    "        response = requests.get(api_url)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Le dépôt {url} est déjà archivé.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Le dépôt {url} n'est pas encore archivé.\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur inconnue lors de la vérification de {url}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Fonction pour soumettre un dépôt à l'archivage sur Software Heritage\n",
    "def submit_for_archiving(url):\n",
    "    api_url = \"https://archive.softwareheritage.org/api/1/origin/save/\"\n",
    "    try:\n",
    "        response = requests.post(api_url, json={\"url\": url})\n",
    "        if response.status_code == 201:\n",
    "            print(f\"Le dépôt {url} a été soumis pour archivage avec succès.\")\n",
    "        else:\n",
    "            print(f\"Erreur lors de la soumission pour {url}: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la soumission pour {url}: {e}\")\n",
    "\n",
    "# Connexion à la base de données SQLite\n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor()\n",
    "\n",
    "# Récupérer les URLs des dépôts dans la base de données\n",
    "c.execute(\"SELECT software_links FROM articles WHERE software_links IS NOT NULL\")\n",
    "urls = c.fetchall()\n",
    "\n",
    "# Boucle pour vérifier et soumettre les dépôts à Software Heritage\n",
    "for url_tuple in urls:\n",
    "    # Extraire l'URL du tuple, et séparer si plusieurs URLs sont présentes\n",
    "    url_list = url_tuple[0].split(', ')  # Séparer par virgule et espace\n",
    "    for url in url_list:\n",
    "        url = url.strip()  # Retirer les espaces en trop\n",
    "        if not check_archived(url):\n",
    "            submit_for_archiving(url)\n",
    "\n",
    "# Fermer la connexion à la base de données\n",
    "database.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
