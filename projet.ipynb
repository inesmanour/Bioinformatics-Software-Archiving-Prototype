{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation de bibliotheques \n",
    "\n",
    "**pip install beautifulsoup4**\n",
    "\n",
    "**pip install requests**\n",
    "\n",
    "**pip install PyPDF2**\n",
    "\n",
    "**pip install pdfplumber**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pdfplumber\n",
    "import re, io\n",
    "import PyPDF2\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraper les articles bioRxiv liés à la bioinformatique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.biorxiv.org'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7fd4109d3f80>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creer base de données SQLite \"bioinformatics_articles\" \n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor() #ici le curseur 'c' permettra d'executer/parcourir les requetes SQL sur database\n",
    "\n",
    "\n",
    "c.execute(\"DROP TABLE IF EXISTS articles\")\n",
    "\n",
    "# create table 'articles' avec col id,title et le lien \n",
    "c.execute('''CREATE TABLE IF NOT EXISTS articles\n",
    "             (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "             title TEXT, \n",
    "             link TEXT,\n",
    "             doi TEXT UNIQUE,\n",
    "             date TEXT,\n",
    "             pdf_link TEXT,\n",
    "             abstract TEXT)''')\n",
    "#id c'est un identifiant unique généré automatiquement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parcourir les articles  de la collection Bioinformatics contenant 2153 pages d'articles et enregistrer leurs informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Nombre de pages à scraper (exemple : 10 pages à la fois)\n",
    "from pickle import TRUE\n",
    "\n",
    "\n",
    "total_pages = 15\n",
    "\n",
    "# boucle qui parcour chaque page de la collection Bioinformatics\n",
    "for page in range(1, total_pages + 1):\n",
    "    print(f\"Scraping page {page} sur {total_pages}\")\n",
    "\n",
    "    # requete qui recup la page bioinfo actuelle\n",
    "    response = requests.get(f'{url}/collection/bioinformatics?page={page}') #? mean numero de la {page}\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Trouver les liens vers les articles sur cette page\n",
    "    articles = soup.find_all('a', class_='highwire-cite-linked-title')  # find liens hypertextes avec classe CSS\n",
    "\n",
    "    print(f'Nombre total d\\'articles trouvés sur la page {page} : {len(articles)}')\n",
    "\n",
    "    # Parcourir tous les articles de la page et enregistrer leurs titres et liens\n",
    "    for article in articles:\n",
    "        title = article.text.strip()  # extraire le titre dans la balise <a>, accessible via .text.\n",
    "        link = url + article['href']  # extraire le lien , href=destination du lien de mon article\n",
    "\n",
    "        # Requête pour accéder à la page de l'article\n",
    "        article_response = requests.get(link)\n",
    "        article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "\n",
    "        # extraire le DOI (dans une balise meta ou dans l'URL)\n",
    "        doi_tag = article_soup.find('meta', {'name': 'citation_doi'})  # balise <meta> qui a l’attribut name=\"citation_doi\".\n",
    "        doi = doi_tag['content'] if doi_tag else 'DOI non disponible'  # si balise existe -> attribut content contient la valeur doi\n",
    "\n",
    "        # extraire la date de publication (dans une balise meta ou time)et la formater au format ISO 8601\n",
    "        date_tag = article_soup.find('meta', {'name': 'citation_publication_date'})\n",
    "        date = date_tag['content'] if date_tag else 'Date non disponible'\n",
    "        # Remplacer les barres obliques par des tirets avant la conversion\n",
    "        if date != 'Date non disponible':\n",
    "            date = date.replace('/', '-')  # Remplacer '/' par '-'\n",
    "            date = datetime.strptime(date, '%Y-%m-%d').date().isoformat()\n",
    "\n",
    "        # extraire le lien vers le fichier PDF\n",
    "        pdf_links = article_soup.find_all('a')\n",
    "        pdf_link = None\n",
    "        for pdf in pdf_links:\n",
    "            if pdf and 'PDF' in pdf.text:\n",
    "                potential_pdf_link = url + pdf['href']\n",
    "                if potential_pdf_link.endswith('.pdf'):  # Vérifier si le lien se termine par .pdf\n",
    "                    pdf_link = potential_pdf_link\n",
    "                    break  # Quitter la boucle une fois le lien trouvé\n",
    "\n",
    "        # En cas de lien invalide, assigner une valeur par défaut\n",
    "        if not pdf_link:\n",
    "            pdf_link = 'Lien PDF non disponible'\n",
    "\n",
    "        \n",
    "        \n",
    "         # Extraire l'abstract\n",
    "        abstract_tag = article_soup.find('meta', {'name': 'citation_abstract'})\n",
    "        if abstract_tag:\n",
    "            abstract_html = abstract_tag['content']\n",
    "            clean_abstract = BeautifulSoup(abstract_html, \"html.parser\").get_text(strip=True)\n",
    "        else:\n",
    "            clean_abstract = 'Abstract non disponible'\n",
    "\n",
    "\n",
    "        # tout inserer dans la base de données SQLite\n",
    "        # Vérifier si l'article existe déjà dans la base de données (vérification sur DOI et date)\n",
    "        c.execute(\"SELECT * FROM articles WHERE doi = ? AND date = ?\", (doi, date))\n",
    "        result = c.fetchone() # recupère la 1ere ligne du result donc si mm doi/date trouvé retourne cette ligne en tuple sinon NONE\n",
    "\n",
    "        if result:\n",
    "            print(f\"L'article avec DOI {doi} existe déjà, ignoré.\")\n",
    "        else:\n",
    "            # Insérer l'article dans la base de données\n",
    "            c.execute(\"INSERT INTO articles (title, link, doi, date, pdf_link, abstract) VALUES (?, ?, ?, ?, ?, ?)\", \n",
    "                      (title, link, doi, date, pdf_link, clean_abstract))\n",
    "            print(f\"Article ajouté : {title}\")\n",
    "\n",
    "        # Afficher les informations extraites\n",
    "        #print(f'Titre: {title}\\nLien: {link}\\nDOI: {doi}\\nDate de publication: {date}\\nLien PDF: {pdf_link}\\nRésumé:{clean_abstract}\\n')\n",
    "        # pause d'une seconde pour éviter de surcharger le serveur\n",
    "        time.sleep(1)\n",
    "        \n",
    "    # Sauvegarder les modifications dans la base de données après chaque page\n",
    "    database.commit()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fermer la connexion à la base de données (quand tu as finis)\n",
    "#database.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extraire les URLs des dépôts logiciels dans les abstracts ou les fichiers PDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tExtraction des liens spécifiques aux dépôts logiciels :\n",
    "\n",
    "La fonction extract_links_from_text() utilise des expressions régulières pour rechercher des URLs dans les textes des résumés et fichiers PDF, notamment des liens vers des dépôts logiciels populaires comme GitHub, GitLab, Bitbucket, SourceForge, ou Zenodo.\n",
    "\n",
    "2.\tÉtape progressive d’extraction:\n",
    "\n",
    "Le script commence par analyser les résumés. Si aucun lien n’est trouvé, il analyse alors le contenu des fichiers PDF des articles pour minimiser les ressources réseau et système.\n",
    "\n",
    "3.\tMise à jour propre de la base de données\n",
    "\n",
    "Seuls les liens valides, nettoyés (pour éviter les erreurs de formatage), et non redondants sont insérés dans la base de données dans la colonne software_links.\n",
    "\n",
    "Évite de retraiter les mêmes articles\n",
    "\n",
    "1.\tVérification des articles traités:\n",
    "\n",
    "Avant de traiter chaque article, le script vérifie si des liens existent déjà dans la colonne software_links. Si des liens sont déjà présents, l’article est ignoré, ce qui permet de ne pas retraiter les articles déjà analysés.\n",
    "\n",
    "2.\tPause après chaque requête:\n",
    "\n",
    "Une pause d’une seconde est introduite après chaque requête pour éviter de surcharger les serveurs web lors de l’extraction des fichiers PDF ou des liens.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdfplumber : Utilisé pour extraire du texte à partir des fichiers PDF.\n",
    "# io : Fournit des outils pour manipuler des fichiers en mémoire (ici, le PDF téléchargé).\n",
    "# re : Module pour travailler avec des expressions régulières, utilisé pour extraire des URLs dans le texte.\n",
    "\n",
    "\n",
    "\n",
    "# Connexion à la base de données SQLite\n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor()\n",
    "\n",
    "# Ajouter une colonne software_links pour stocker les liens de dépôt logiciel si elle n'existe pas\n",
    "try:\n",
    "    c.execute(\"ALTER TABLE articles ADD COLUMN software_links TEXT\")\n",
    "    database.commit()\n",
    "except sqlite3.OperationalError:  # colonne existe déjà \n",
    "    pass  # La colonne existe déjà\n",
    "\n",
    "# Fonction pour vérifier si un lien est valide\n",
    "def is_valid_link(link):\n",
    "    return link.startswith('http://') or link.startswith('https://')\n",
    "\n",
    "# Fonction pour nettoyer les liens partiels ou incorrects\n",
    "def clean_link(link):\n",
    "    link = link.strip(').;,')  # Supprimer les parenthèses et les points en fin de lien\n",
    "    return link if is_valid_link(link) else None\n",
    "\n",
    "# Fonction pour extraire les liens à partir d'un texte\n",
    "def extract_links_from_text(text):\n",
    "    return re.findall(r'https?://\\S+', text)\n",
    "\n",
    "\n",
    "#fonction principale\n",
    "# Fonction pour extraire les liens de dépôt logiciel\n",
    "def extract_software_links():\n",
    "    # Récupérer tous les articles de la base de données\n",
    "    c.execute(\"SELECT id, title, pdf_link, abstract, software_links FROM articles\")\n",
    "    articles = c.fetchall() #récupère tous les articles \n",
    "\n",
    "    for article in articles:\n",
    "        article_id, title, pdf_link, abstract, existing_links = article\n",
    "\n",
    "        # Si l'article a déjà des liens de dépôt logiciel, on l'ignore\n",
    "        if existing_links:\n",
    "            print(f\"Article {article_id} déjà traité, on passe au suivant.\")\n",
    "            continue\n",
    "\n",
    "        software_links_set = set()  # Utiliser un set pour éliminer les doublons\n",
    "\n",
    "            \n",
    "            #extraire lien a partir du pdf\n",
    "        # Vérifier le lien PDF\n",
    "        if pdf_link and pdf_link != 'Lien PDF non disponible':\n",
    "            try:\n",
    "                # Télécharger le fichier PDF\n",
    "                response = requests.get(pdf_link)\n",
    "                response.raise_for_status()  # Vérifie si la requête a réussi\n",
    "\n",
    "                # Utiliser pdfplumber pour extraire le texte du PDF\n",
    "                with pdfplumber.open(io.BytesIO(response.content)) as pdf:\n",
    "                    for page in pdf.pages:\n",
    "                        text = page.extract_text()\n",
    "                        if text:\n",
    "                            # Rechercher des liens de dépôt logiciel dans le texte\n",
    "                            software_links_set.update(extract_links_from_text(text))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de l'ouverture du PDF pour l'article {article_id}: {e}\")\n",
    "\n",
    "        # Si aucun lien n'a été trouvé dans le PDF, vérifier l'abstract\n",
    "        if not software_links_set and abstract != 'Abstract non disponible':\n",
    "            software_links_set.update(extract_links_from_text(abstract))\n",
    "\n",
    "        # Nettoyage des liens et validation\n",
    "        valid_links = [clean_link(link) for link in software_links_set if clean_link(link)]\n",
    "            #lien trouvés sont nettoyés et valide par clean_link() et is_valid_link().\n",
    "\n",
    "        # Mise à jour de la base de données avec les liens de dépôt logiciel\n",
    "        #liens valides trouvés, ajoutés à la base de données dans la colonne software_links \n",
    "        if valid_links:\n",
    "            print(f\"Liens de dépôt logiciel valides pour l'article {article_id} : {valid_links}\")\n",
    "            c.execute(\"UPDATE articles SET software_links = ? WHERE id = ?\", (', '.join(valid_links), article_id))\n",
    "        else:\n",
    "            print(f\"Aucun lien de dépôt logiciel trouvé pour l'article {article_id}\")\n",
    "\n",
    "        time.sleep(1)  # Pause pour éviter de surcharger le serveur\n",
    "\n",
    "    # Sauvegarder les modifications dans la base de données\n",
    "    database.commit()\n",
    "\n",
    "# Appeler la fonction d'extraction\n",
    "extract_software_links()\n",
    "\n",
    "# Fermer la connexion à la base de données\n",
    "#database.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Affichage de mes articles dans la base de données (A supprimer c'est juste pr que je vois bien l'ajout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import sqlite3\n",
    "\n",
    "# Connexion à la base de données SQLite\n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor()\n",
    "\n",
    "# Requête pour récupérer les informations de la table articles\n",
    "try:\n",
    "    c.execute(\"SELECT * FROM articles\")\n",
    "    rows = c.fetchall()\n",
    "    \n",
    "    # Afficher le contenu de la table articles\n",
    "    if rows:\n",
    "        for row in rows:\n",
    "            print(row)\n",
    "    else:\n",
    "        print(\"La table articles est vide ou n'existe pas.\")\n",
    "\n",
    "except sqlite3.OperationalError as e:\n",
    "    print(f\"Erreur lors de la récupération des données : {e}\")\n",
    "\n",
    "# Fermer la connexion à la base de données\n",
    "#database.close() '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# suite du projet  :\n",
    "\n",
    "1.\t Vérification des URLs:\n",
    "\n",
    "Tester la validité des URLs extraites en effectuant des requêtes HTTP pour s’assurer qu’elles sont accessibles et correspondent à des dépôts logiciels actifs.\n",
    "\n",
    "2.\tArchivage via Software Heritage:\n",
    "\n",
    "Utiliser l’API de Software Heritage pour vérifier si le dépôt logiciel est déjà archivé dans leur base de données. Si le dépôt n’est pas encore archivé, soumettre automatiquement une demande d’archivage via l’API.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
