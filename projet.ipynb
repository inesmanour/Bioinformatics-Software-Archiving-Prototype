{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation de bibliotheques \n",
    "\n",
    "**pip install beautifulsoup4**\n",
    "\n",
    "**pip install requests**\n",
    "\n",
    "**pip install PyPDF2**\n",
    "\n",
    "**pip install pdfplumber**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pdfplumber\n",
    "import re, io\n",
    "import PyPDF2\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraper les articles bioRxiv liés à la bioinformatique "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parcourir les articles de la collection Bioinformatics contenant au total 2153 pages d'articles, ainsi scraper les 10 pages à chaque exécution en enregistrant la dernière page scrappée et leurs informations dans la database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "url = 'https://www.biorxiv.org'\n",
    "\n",
    "# créer la base de données SQLite \"bioinformatics_articles\" \n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor() \n",
    "\n",
    "# créer la table 'articles' si elle n'existe pas \n",
    "c.execute('''CREATE TABLE IF NOT EXISTS articles\n",
    "             (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "             title TEXT, \n",
    "             link TEXT,\n",
    "             doi TEXT UNIQUE,\n",
    "             date TEXT,\n",
    "             pdf_link TEXT,\n",
    "             abstract TEXT)''')\n",
    "\n",
    "# Créer une table pour stocker la dernière page scrappée\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS scrap_info (\n",
    "             last_page INTEGER)''')\n",
    "\n",
    "# Récupérer la dernière page scrappée\n",
    "c.execute(\"SELECT last_page FROM scrap_info\")\n",
    "result = c.fetchone()\n",
    "last_page = result[0] if result else 0  # Si aucune page n'a été scrappée, commencer à la page 1\n",
    "start_page = last_page + 1\n",
    "\n",
    "## Nombre de pages à scraper (exemple : 10 pages à la fois)\n",
    "pages_to_scrap_per_run = 10\n",
    "total_pages = min(start_page + pages_to_scrap_per_run - 1, 2153)  # Limiter à 2153 pages\n",
    "\n",
    "# boucle qui parcourt chaque page de la collection Bioinformatics\n",
    "for page in range(start_page, total_pages + 1):\n",
    "    print(f\"Scraping page {page} sur {total_pages}\")\n",
    "\n",
    "    # requête qui récupère la page bioinfo actuelle\n",
    "    response = requests.get(f'{url}/collection/bioinformatics?page={page}')\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Trouver les liens vers les articles sur cette page\n",
    "    articles = soup.find_all('a', class_='highwire-cite-linked-title')\n",
    "    print(f'Nombre total d\\'articles trouvés sur la page {page} : {len(articles)}')\n",
    "\n",
    "    # Parcourir tous les articles de la page et enregistrer leurs titres et liens\n",
    "    for article in articles:\n",
    "        title = article.text.strip()\n",
    "        link = url + article['href']\n",
    "\n",
    "        # Requête pour accéder à la page de l'article\n",
    "        article_response = requests.get(link)\n",
    "        article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "\n",
    "        # extraire le DOI\n",
    "        doi_tag = article_soup.find('meta', {'name': 'citation_doi'})\n",
    "        doi = doi_tag['content'] if doi_tag else 'DOI non disponible'\n",
    "\n",
    "        # extraire la date de publication\n",
    "        date_tag = article_soup.find('meta', {'name': 'citation_publication_date'})\n",
    "        date = date_tag['content'] if date_tag else 'Date non disponible'\n",
    "        if date != 'Date non disponible':\n",
    "            date = date.replace('/', '-')\n",
    "            date = datetime.strptime(date, '%Y-%m-%d').date().isoformat()\n",
    "\n",
    "        # extraire le lien vers le fichier PDF\n",
    "        pdf_links = article_soup.find_all('a')\n",
    "        pdf_link = next((url + pdf['href'] for pdf in pdf_links if 'PDF' in pdf.text and pdf['href'].endswith('.pdf')), 'Lien PDF non disponible')\n",
    "\n",
    "        # Extraire l'abstract\n",
    "        abstract_tag = article_soup.find('meta', {'name': 'citation_abstract'})\n",
    "        clean_abstract = abstract_tag['content'] if abstract_tag else 'Abstract non disponible'\n",
    "\n",
    "        # Vérifier si l'article existe déjà dans la base de données\n",
    "        c.execute(\"SELECT * FROM articles WHERE doi = ? AND date = ?\", (doi, date))\n",
    "        result = c.fetchone()\n",
    "\n",
    "        if result:\n",
    "            print(f\"L'article avec DOI {doi} existe déjà, ignoré.\")\n",
    "        else:\n",
    "            # Insérer l'article dans la base de données\n",
    "            c.execute(\"INSERT INTO articles (title, link, doi, date, pdf_link, abstract) VALUES (?, ?, ?, ?, ?, ?)\", \n",
    "                      (title, link, doi, date, pdf_link, clean_abstract))\n",
    "            print(f\"Article ajouté : {title}\")\n",
    "\n",
    "        time.sleep(1)  # pause d'une seconde pour éviter de surcharger le serveur\n",
    "\n",
    "    # Sauvegarder les modifications dans la base de données après chaque page\n",
    "    database.commit()\n",
    "\n",
    "    # Mettre à jour la dernière page scrappée dans la table 'scrap_info'\n",
    "    c.execute(\"UPDATE scrap_info SET last_page = ?\", (page,))\n",
    "    if c.rowcount == 0:  \n",
    "        c.execute(\"INSERT INTO scrap_info (last_page) VALUES (?)\", (page,))\n",
    "\n",
    "# Sauvegarder la dernière page dans la base de données\n",
    "database.commit()\n",
    "\n",
    "# Fermer la connexion à la base de données\n",
    "database.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extraire les URLs des dépôts logiciels(github and gitlab) dans les abstracts ET les fichiers PDF si url valide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Connexion à la base de données SQLite\n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor()\n",
    "\n",
    "# Vérifier et ajouter les colonnes software_links et processed si elles n'existent pas\n",
    "try:\n",
    "    c.execute(\"ALTER TABLE articles ADD COLUMN software_links TEXT\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass  # La colonne existe déjà\n",
    "\n",
    "try:\n",
    "    c.execute(\"ALTER TABLE articles ADD COLUMN processed INTEGER DEFAULT 0\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass  # La colonne existe déjà\n",
    "\n",
    "# Fonction pour vérifier si un lien est valide et s'il est un lien GitHub ou GitLab\n",
    "def is_valid_link(link):\n",
    "    return link.startswith('http://') or link.startswith('https://')\n",
    "\n",
    "def is_github_or_gitlab(link):\n",
    "    return 'github.com' in link or 'gitlab.com' in link\n",
    "\n",
    "# Fonction pour nettoyer les liens partiels ou incorrects\n",
    "def clean_link(link):\n",
    "    link = link.strip(').;,[]')  # Supprimer les caractères indésirables\n",
    "    if is_valid_link(link) and len(link) > len('https://github.com/'):\n",
    "        return link\n",
    "    return None\n",
    "\n",
    "# Fonction pour tester la validité des URLs via une requête HTTP\n",
    "def check_url_validity(link):\n",
    "    try:\n",
    "        response = requests.get(link, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Le lien est valide : {link}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Le lien a retourné un code de statut {response.status_code} pour : {link}\")\n",
    "            return False\n",
    "    except requests.RequestException:\n",
    "        print(f\"Erreur lors de la vérification du lien : {link}\")\n",
    "        return False\n",
    "\n",
    "# Fonction pour extraire les URLs des dépôts logiciels\n",
    "def extract_repository_urls(text):\n",
    "    url_pattern = r'(https?://[^\\s]+)'\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    repo_urls = []\n",
    "\n",
    "    for url in urls:\n",
    "        cleaned_url = clean_link(url)\n",
    "        if cleaned_url and is_github_or_gitlab(cleaned_url):\n",
    "            if check_url_validity(cleaned_url):  # Vérification de l'URL avant de l'ajouter\n",
    "                repo_urls.append(cleaned_url)\n",
    "\n",
    "    return repo_urls\n",
    "\n",
    "# Fonction pour extraire les liens de dépôt logiciel\n",
    "def extract_software_links(article_id, pdf_link, abstract):\n",
    "    software_links_set = set()\n",
    "\n",
    "    if pdf_link and pdf_link != 'Lien PDF non disponible':\n",
    "        try:\n",
    "            print(f\"Ouverture du PDF pour l'article {article_id}: {pdf_link}\")\n",
    "            response = requests.get(pdf_link)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            with pdfplumber.open(io.BytesIO(response.content)) as pdf:\n",
    "                print(f\"Le PDF a été ouvert avec succès pour l'article {article_id}. Recherche de liens...\")\n",
    "                for page in pdf.pages:\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        links_from_pdf = extract_repository_urls(text)\n",
    "                        if links_from_pdf:\n",
    "                            software_links_set.update(links_from_pdf)\n",
    "                            print(f\"Liens trouvés dans le PDF pour l'article {article_id} : {links_from_pdf}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'ouverture du PDF pour l'article {article_id}: {e}\")\n",
    "\n",
    "    if abstract and abstract != 'Abstract non disponible':\n",
    "        print(f\"Vérification de l'abstract pour l'article {article_id}.\")\n",
    "        links_from_abstract = extract_repository_urls(abstract)\n",
    "        if links_from_abstract:\n",
    "            software_links_set.update(links_from_abstract)\n",
    "            print(f\"Liens trouvés dans l'abstract pour l'article {article_id} : {links_from_abstract}\")\n",
    "\n",
    "    valid_links = [clean_link(link) for link in software_links_set if clean_link(link) and is_github_or_gitlab(link)]\n",
    "\n",
    "    if valid_links:\n",
    "        print(f\"Liens de dépôt logiciel valides pour l'article {article_id} : {valid_links}\")\n",
    "        c.execute(\"UPDATE articles SET software_links = ?, processed = 1 WHERE id = ?\", (', '.join(valid_links), article_id))\n",
    "        print(f\"Liens ajoutés à la base de données pour l'article {article_id}.\")\n",
    "    else:\n",
    "        print(f\"Aucun lien de dépôt logiciel trouvé pour l'article {article_id}.\")\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "# Fonction pour traiter uniquement les nouveaux articles\n",
    "def process_new_articles():\n",
    "    c.execute(\"SELECT id, title, pdf_link, abstract FROM articles WHERE processed = 0\")\n",
    "    articles_to_process = c.fetchall()\n",
    "\n",
    "    for article in articles_to_process:\n",
    "        article_id, title, pdf_link, abstract = article\n",
    "        print(f\"Traitement de l'article : {title} (ID: {article_id})...\")\n",
    "        extract_software_links(article_id, pdf_link, abstract)\n",
    "\n",
    "# Appel de la fonction principale\n",
    "try:\n",
    "    process_new_articles()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Traitement interrompu. Les données sont sauvegardées.\")\n",
    "finally:\n",
    "    database.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Affichage de mes articles dans la base de données (A supprimer c'est juste pr que je vois bien l'ajout sur jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le faire en cas de soucis and need tout recommencer \n",
    "#c.execute(\"DROP TABLE IF EXISTS articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connexion à la base de données SQLite\n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor()\n",
    "\n",
    "# Fonction pour afficher les attributs de la table articles\n",
    "def display_articles():\n",
    "    # Exécuter une requête pour récupérer toutes les colonnes et enregistrements de la table articles\n",
    "    c.execute(\"SELECT * FROM articles\")\n",
    "    \n",
    "    # Récupérer les noms de colonnes\n",
    "    column_names = [description[0] for description in c.description]\n",
    "    \n",
    "    # Afficher les noms des colonnes\n",
    "    print(f\"{' | '.join(column_names)}\")\n",
    "    print(\"-\" * (len(column_names) * 4))  # Ligne de séparation\n",
    "    \n",
    "    # Récupérer et afficher tous les enregistrements\n",
    "    for row in c.fetchall():\n",
    "        print(' | '.join(map(str, row)))\n",
    "\n",
    "# Appeler la fonction pour afficher les articles\n",
    "display_articles()\n",
    "\n",
    "# Fermer la connexion à la base de données\n",
    "database.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# suite du projet  :\n",
    "\n",
    "2.\tArchivage via Software Heritage:\n",
    "\n",
    "Utiliser l’API de Software Heritage pour vérifier si le dépôt logiciel est déjà archivé dans leur base de données. Si le dépôt n’est pas encore archivé, soumettre automatiquement une demande d’archivage via l’API.\n",
    "\n",
    "--->Script 3 : lire la BDD, récupérer les URL, vérifier que le dépôt est archivé dans Software Heritage\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
