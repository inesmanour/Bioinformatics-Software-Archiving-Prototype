{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation de bibliotheques \n",
    "\n",
    "**pip install beautifulsoup4**\n",
    "\n",
    "**pip install requests**\n",
    "\n",
    "**pip install PyPDF2**\n",
    "\n",
    "**pip install pdfplumber**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pdfplumber\n",
    "import re, io\n",
    "import PyPDF2\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "import time\n",
    "from threading import Timer\n",
    "\n",
    "import logging\n",
    "import urllib.parse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraper les articles bioRxiv liés à la bioinformatique "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parcourir les articles de la collection Bioinformatics , scraper 10 pages à chaque exécution . (run toutes les semaines puisque new articles se mette en page 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'https://www.biorxiv.org'\n",
    "\n",
    "# Créer la base de données SQLite \"bioinformatics_articles\"\n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor()\n",
    "\n",
    "# Créer la table 'articles' si elle n'existe pas\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS articles\n",
    "             (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "             title TEXT, \n",
    "             link TEXT,\n",
    "             doi TEXT UNIQUE,\n",
    "             date TEXT,\n",
    "             pdf_link TEXT,\n",
    "             abstract TEXT)''')\n",
    "\n",
    "# Nombre de pages à scraper (exemple : 10 pages à la fois)\n",
    "total_pages = 10\n",
    "\n",
    "# Boucle qui parcourt chaque page de la collection Bioinformatics\n",
    "for page in range(1, total_pages + 1):\n",
    "    print(f\"Scraping page {page} sur {total_pages}\")\n",
    "\n",
    "    # Requête qui récupère la page bioinfo actuelle\n",
    "    response = requests.get(f'{url}/collection/bioinformatics?page={page}')\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Trouver les liens vers les articles sur cette page\n",
    "    articles = soup.find_all('a', class_='highwire-cite-linked-title')\n",
    "\n",
    "    print(f'Nombre total d\\'articles trouvés sur la page {page} : {len(articles)}')\n",
    "\n",
    "    # Parcourir tous les articles de la page et enregistrer leurs titres et liens\n",
    "    for article in articles:\n",
    "        title = article.text.strip()\n",
    "        link = url + article['href']\n",
    "\n",
    "        # Requête pour accéder à la page de l'article\n",
    "        article_response = requests.get(link)\n",
    "        article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "\n",
    "        # Extraire le DOI\n",
    "        doi_tag = article_soup.find('meta', {'name': 'citation_doi'})\n",
    "        doi = doi_tag['content'] if doi_tag else 'DOI non disponible'\n",
    "\n",
    "        # Extraire la date de publication\n",
    "        date_tag = article_soup.find('meta', {'name': 'citation_publication_date'})\n",
    "        date = date_tag['content'] if date_tag else 'Date non disponible'\n",
    "        if date != 'Date non disponible':\n",
    "            date = date.replace('/', '-')  # Remplacer '/' par '-'\n",
    "            date = datetime.strptime(date, '%Y-%m-%d').date().isoformat()\n",
    "\n",
    "        # Extraire le lien vers le fichier PDF\n",
    "        pdf_links = article_soup.find_all('a')\n",
    "        pdf_link = next((url + pdf['href'] for pdf in pdf_links if 'PDF' in pdf.text and pdf['href'].endswith('.pdf')), 'Lien PDF non disponible')\n",
    "\n",
    "        # Extraire l'abstract\n",
    "        abstract_tag = article_soup.find('meta', {'name': 'citation_abstract'})\n",
    "        clean_abstract = abstract_tag['content'] if abstract_tag else 'Abstract non disponible'\n",
    "\n",
    "        # Supprimer les balises <p> au début et à la fin de l'abstract\n",
    "        if clean_abstract.startswith('<p>'):\n",
    "            clean_abstract = clean_abstract[3:]  # Supprimer les 3 premiers caractères\n",
    "        if clean_abstract.endswith('</p>'):\n",
    "            clean_abstract = clean_abstract[:-4]  # Supprimer les 4 derniers caractères\n",
    "\n",
    "        # Insérer dans la base de données SQLite\n",
    "        c.execute(\"SELECT * FROM articles WHERE doi = ? AND date = ?\", (doi, date))\n",
    "        result = c.fetchone()\n",
    "\n",
    "        if result:\n",
    "            print(f\"L'article avec DOI {doi} existe déjà, ignoré.\")\n",
    "        else:\n",
    "            c.execute(\"INSERT INTO articles (title, link, doi, date, pdf_link, abstract) VALUES (?, ?, ?, ?, ?, ?)\", \n",
    "                      (title, link, doi, date, pdf_link, clean_abstract))\n",
    "            print(f\"Article ajouté : {title}\")\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Sauvegarder les modifications dans la base de données après chaque page\n",
    "    database.commit()\n",
    "    \n",
    "database.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extraction des URLs des Dépôts Logiciels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1.\tVérification et ajout des colonnes nécessaires dans la base de données.\n",
    "\n",
    "2.\tValidation des liens pour s’assurer qu’ils sont valides et pertinents.\n",
    "\n",
    "3.\tVérification de l’accessibilité des URLs via des requêtes HTTP.\n",
    "\n",
    "4.\tExtraction des URLs des dépôts à partir des contenus des PDF et des abstracts.\n",
    "\n",
    "5.\tTraitement des nouveaux articles non encore traités pour l’extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les articles qui ont été vérifiés, mais sans trouver de liens logiciels, seront mis à jour avec is_article_processed = 2. Cela permet de ne pas les vérifier à nouveau lors des exécutions futures.---> Le script ne traite que les articles où is_article_processed = 0. Cela évite de revérifier les articles déjà traités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Connexion à la base de données SQLite\n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor()\n",
    "\n",
    "# Vérifier et ajouter les colonnes software_links et is_article_processed si elles n'existent pas\n",
    "try:\n",
    "    c.execute(\"ALTER TABLE articles ADD COLUMN software_links TEXT\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass  # La colonne existe déjà\n",
    "\n",
    "try:\n",
    "    c.execute(\"ALTER TABLE articles ADD COLUMN is_article_processed INTEGER DEFAULT 0\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass  # La colonne existe déjà\n",
    "\n",
    "# Fonction pour vérifier si un lien est valide et s'il est un lien GitHub ou GitLab\n",
    "def is_valid_link(link):\n",
    "    return link.startswith('http://') or link.startswith('https://')\n",
    "\n",
    "def is_github_or_gitlab(link):\n",
    "    return 'github.com' in link or 'gitlab.com' in link\n",
    "\n",
    "# Fonction pour nettoyer les liens partiels ou incorrects\n",
    "def clean_link(link):\n",
    "    link = link.strip(').;,[]')  # Supprimer les caractères indésirables\n",
    "    if is_valid_link(link) and len(link) > len('https://github.com/'):\n",
    "        return link\n",
    "    return None\n",
    "\n",
    "# Fonction pour tester la validité des URLs via une requête HTTP\n",
    "def check_url_validity(link):\n",
    "    try:\n",
    "        response = requests.get(link, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Le lien est valide : {link}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Le lien a retourné un code de statut {response.status_code} pour : {link}\")\n",
    "            return False\n",
    "    except requests.RequestException:\n",
    "        print(f\"Erreur lors de la vérification du lien : {link}\")\n",
    "        return False\n",
    "\n",
    "# Fonction pour extraire les URLs des dépôts logiciels\n",
    "def extract_repository_urls(text):\n",
    "    url_pattern = r'(https?://[^\\s]+)'\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    repo_urls = []\n",
    "\n",
    "    for url in urls:\n",
    "        cleaned_url = clean_link(url)\n",
    "        if cleaned_url and is_github_or_gitlab(cleaned_url):\n",
    "            if check_url_validity(cleaned_url):  # Vérification de l'URL avant de l'ajouter\n",
    "                repo_urls.append(cleaned_url)\n",
    "\n",
    "    return repo_urls\n",
    "\n",
    "# Fonction pour extraire les liens de dépôt logiciel\n",
    "def extract_software_links(article_id, pdf_link, abstract):\n",
    "    software_links_set = set()\n",
    "\n",
    "    # Extraction des liens depuis le PDF\n",
    "    if pdf_link and pdf_link != 'Lien PDF non disponible':\n",
    "        try:\n",
    "            print(f\"Ouverture du PDF pour l'article {article_id}: {pdf_link}\")\n",
    "            response = requests.get(pdf_link)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            with pdfplumber.open(io.BytesIO(response.content)) as pdf:\n",
    "                print(f\"Le PDF a été ouvert avec succès pour l'article {article_id}. Recherche de liens...\")\n",
    "                for page in pdf.pages:\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        links_from_pdf = extract_repository_urls(text)\n",
    "                        if links_from_pdf:\n",
    "                            software_links_set.update(links_from_pdf)\n",
    "                            print(f\"Liens trouvés dans le PDF pour l'article {article_id} : {links_from_pdf}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'ouverture du PDF pour l'article {article_id}: {e}\")\n",
    "\n",
    "    # Extraction des liens depuis l'abstract\n",
    "    if abstract and abstract != 'Abstract non disponible':\n",
    "        print(f\"Vérification de l'abstract pour l'article {article_id}.\")\n",
    "        links_from_abstract = extract_repository_urls(abstract)\n",
    "        if links_from_abstract:\n",
    "            software_links_set.update(links_from_abstract)\n",
    "            print(f\"Liens trouvés dans l'abstract pour l'article {article_id} : {links_from_abstract}\")\n",
    "\n",
    "    valid_links = [clean_link(link) for link in software_links_set if clean_link(link) and is_github_or_gitlab(link)]\n",
    "\n",
    "    # Mise à jour de la base de données selon la présence ou non de liens valides\n",
    "    if valid_links:\n",
    "        print(f\"Liens de dépôt logiciel valides pour l'article {article_id} : {valid_links}\")\n",
    "        c.execute(\"UPDATE articles SET software_links = ?, is_article_processed = 1 WHERE id = ?\", (', '.join(valid_links), article_id))\n",
    "    else:\n",
    "        print(f\"Aucun lien de dépôt logiciel trouvé pour l'article {article_id}.\")\n",
    "        c.execute(\"UPDATE articles SET is_article_processed = 2 WHERE id = ?\", (article_id,))\n",
    "\n",
    "    # Enregistrer les modifications dans la base de données\n",
    "    database.commit()\n",
    "    time.sleep(1)\n",
    "\n",
    "# Fonction pour traiter uniquement les nouveaux articles non traités (is_article_processed = 0)\n",
    "def process_new_articles():\n",
    "    c.execute(\"SELECT id, title, pdf_link, abstract FROM articles WHERE is_article_processed = 0\")\n",
    "    articles_to_process = c.fetchall()\n",
    "\n",
    "    # Vérifier s'il y a des articles à traiter\n",
    "    if not articles_to_process:\n",
    "        print(\"Aucun article à traiter.\")\n",
    "    else:\n",
    "        print(f\"{len(articles_to_process)} article(s) trouvé(s) à traiter.\")\n",
    "\n",
    "    for article in articles_to_process:\n",
    "        article_id, title, pdf_link, abstract = article\n",
    "        print(f\"Traitement de l'article : {title} (ID: {article_id})...\")\n",
    "        extract_software_links(article_id, pdf_link, abstract)\n",
    "\n",
    "# Appel de la fonction principale\n",
    "try:\n",
    "    process_new_articles()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Traitement interrompu. Les données sont sauvegardées.\")\n",
    "finally:\n",
    "    database.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliser ce code lorsqu'un article avec un temps de recherche dans le pdf trop long ,timer définit a 5min et passe après a l'abstract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Connexion à la base de données SQLite\n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor()\n",
    "\n",
    "# Vérifier et ajouter les colonnes software_links et is_article_processed si elles n'existent pas\n",
    "try:\n",
    "    c.execute(\"ALTER TABLE articles ADD COLUMN software_links TEXT\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass  # La colonne existe déjà\n",
    "\n",
    "try:\n",
    "    c.execute(\"ALTER TABLE articles ADD COLUMN is_article_processed INTEGER DEFAULT 0\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass  # La colonne existe déjà\n",
    "\n",
    "# Fonction pour vérifier si un lien est valide et s'il est un lien GitHub ou GitLab\n",
    "def is_valid_link(link):\n",
    "    return link.startswith('http://') or link.startswith('https://')\n",
    "\n",
    "def is_github_or_gitlab(link):\n",
    "    return 'github.com' in link or 'gitlab.com' in link\n",
    "\n",
    "# Fonction pour nettoyer les liens partiels ou incorrects\n",
    "def clean_link(link):\n",
    "    link = link.strip(').;,[]')  # Supprimer les caractères indésirables\n",
    "    if is_valid_link(link) and len(link) > len('https://github.com/'):\n",
    "        return link\n",
    "    return None\n",
    "\n",
    "# Fonction pour tester la validité des URLs via une requête HTTP\n",
    "def check_url_validity(link):\n",
    "    try:\n",
    "        response = requests.get(link, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Le lien est valide : {link}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Le lien a retourné un code de statut {response.status_code} pour : {link}\")\n",
    "            return False\n",
    "    except requests.RequestException:\n",
    "        print(f\"Erreur lors de la vérification du lien : {link}\")\n",
    "        return False\n",
    "\n",
    "# Fonction pour extraire les URLs des dépôts logiciels\n",
    "def extract_repository_urls(text):\n",
    "    url_pattern = r'(https?://[^\\s]+)'\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    repo_urls = []\n",
    "\n",
    "    for url in urls:\n",
    "        cleaned_url = clean_link(url)\n",
    "        if cleaned_url and is_github_or_gitlab(cleaned_url):\n",
    "            if check_url_validity(cleaned_url):  # Vérification de l'URL avant de l'ajouter\n",
    "                repo_urls.append(cleaned_url)\n",
    "\n",
    "    return repo_urls\n",
    "\n",
    "# Fonction pour extraire les liens de dépôt logiciel\n",
    "def extract_software_links(article_id, pdf_link, abstract):\n",
    "    software_links_set = set()\n",
    "\n",
    "    # Timer pour contrôler le temps d'attente\n",
    "    timer = None\n",
    "    \n",
    "    def time_up():\n",
    "        nonlocal timer\n",
    "        print(f\"Temps écoulé pour l'article {article_id}. Passage à l'abstract.\")\n",
    "        timer = None  # Annule le timer\n",
    "\n",
    "    # Extraction des liens depuis le PDF\n",
    "    if pdf_link and pdf_link != 'Lien PDF non disponible':\n",
    "        timer = Timer(300, time_up)  # 60 secondes = 1 minute\n",
    "        timer.start()\n",
    "\n",
    "        try:\n",
    "            print(f\"Ouverture du PDF pour l'article {article_id}: {pdf_link}\")\n",
    "            response = requests.get(pdf_link)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            with pdfplumber.open(io.BytesIO(response.content)) as pdf:\n",
    "                print(f\"Le PDF a été ouvert avec succès pour l'article {article_id}. Recherche de liens...\")\n",
    "                for page in pdf.pages:\n",
    "                    if timer is None:  # Vérifie si le timer est toujours actif\n",
    "                        text = page.extract_text()\n",
    "                        if text:\n",
    "                            links_from_pdf = extract_repository_urls(text)\n",
    "                            if links_from_pdf:\n",
    "                                software_links_set.update(links_from_pdf)\n",
    "                                print(f\"Liens trouvés dans le PDF pour l'article {article_id} : {links_from_pdf}\")\n",
    "                                break  # Sortir de la boucle si des liens sont trouvés\n",
    "                    else:\n",
    "                        print(\"Recherche dans le PDF abandonnée en raison du dépassement de temps.\")\n",
    "                        break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'ouverture du PDF pour l'article {article_id}: {e}\")\n",
    "\n",
    "        if timer is not None:\n",
    "            timer.cancel()  # Annuler le timer si l'extraction a réussi\n",
    "\n",
    "    # Extraction des liens depuis l'abstract\n",
    "    if not software_links_set and abstract and abstract != 'Abstract non disponible':\n",
    "        print(f\"Vérification de l'abstract pour l'article {article_id}.\")\n",
    "        links_from_abstract = extract_repository_urls(abstract)\n",
    "        if links_from_abstract:\n",
    "            software_links_set.update(links_from_abstract)\n",
    "            print(f\"Liens trouvés dans l'abstract pour l'article {article_id} : {links_from_abstract}\")\n",
    "\n",
    "    valid_links = [clean_link(link) for link in software_links_set if clean_link(link) and is_github_or_gitlab(link)]\n",
    "\n",
    "    # Mise à jour de la base de données selon la présence ou non de liens valides\n",
    "    if valid_links:\n",
    "        print(f\"Liens de dépôt logiciel valides pour l'article {article_id} : {valid_links}\")\n",
    "        c.execute(\"UPDATE articles SET software_links = ?, is_article_processed = 1 WHERE id = ?\", (', '.join(valid_links), article_id))\n",
    "    else:\n",
    "        print(f\"Aucun lien de dépôt logiciel trouvé pour l'article {article_id}.\")\n",
    "        c.execute(\"UPDATE articles SET is_article_processed = 2 WHERE id = ?\", (article_id,))\n",
    "\n",
    "    # Enregistrer les modifications dans la base de données\n",
    "    database.commit()\n",
    "    time.sleep(1)\n",
    "\n",
    "# Fonction pour traiter uniquement les nouveaux articles non traités (is_article_processed = 0)\n",
    "def process_new_articles():\n",
    "    c.execute(\"SELECT id, title, pdf_link, abstract FROM articles WHERE is_article_processed = 0\")\n",
    "    articles_to_process = c.fetchall()\n",
    "\n",
    "    # Vérifier s'il y a des articles à traiter\n",
    "    if not articles_to_process:\n",
    "        print(\"Aucun article à traiter.\")\n",
    "    else:\n",
    "        print(f\"{len(articles_to_process)} article(s) trouvé(s) à traiter.\")\n",
    "\n",
    "    for article in articles_to_process:\n",
    "        article_id, title, pdf_link, abstract = article\n",
    "        print(f\"Traitement de l'article : {title} (ID: {article_id})...\")\n",
    "        extract_software_links(article_id, pdf_link, abstract)\n",
    "\n",
    "# Appel de la fonction principale\n",
    "try:\n",
    "    process_new_articles()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Traitement interrompu. Les données sont sauvegardées.\")\n",
    "finally:\n",
    "    database.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.\tArchivage via Software Heritage:\n",
    "\n",
    "## Objectif\n",
    "Utiliser l’API de Software Heritage pour vérifier si le dépôt logiciel est déjà archivé dans leur base de données. Si le dépôt n’est pas encore archivé, soumettre automatiquement une demande d’archivage via l’API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 23:41:05,577 - INFO - Nouvelle base de données 'new_bioinformatics_articles.db' créée avec succès avec la table 'articles'.\n",
      "2024-11-01 23:41:05,585 - WARNING - Les colonnes existent peut-être déjà : duplicate column name: date_last_archive\n",
      "2024-11-01 23:41:05,594 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2FPeterZZQ%2FscMoMaT%2Fblob%2Fmain%2Fdemo_scmomat.ipynb/get/\n",
      "2024-11-01 23:41:06,102 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2FPeterZZQ%2FscMoMaT%2Fblob%2Fmain%2Fdemo_scmomat.ipynb/, Date de la dernière archive : 2024-11-01T22:04:58.927000+00:00\n",
      "2024-11-01 23:41:06,106 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2FPeterZZQ%2FscMoMaT%2Fblob%2Fmain%2Fdemo_scmomat.ipynb/ à la date 2024-11-01T22:04:58.927000+00:00\n",
      "2024-11-01 23:41:06,107 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2FMarioniLab%2FStabMap%2Ftree%2Fmain%2Fvignettes/get/\n",
      "2024-11-01 23:41:06,502 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2FMarioniLab%2FStabMap%2Ftree%2Fmain%2Fvignettes/, Date de la dernière archive : 2024-11-01T22:04:58.874000+00:00\n",
      "2024-11-01 23:41:06,505 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2FMarioniLab%2FStabMap%2Ftree%2Fmain%2Fvignettes/ à la date 2024-11-01T22:04:58.874000+00:00\n",
      "2024-11-01 23:41:06,506 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fepurdom%2Fcobolt%2Fblob%2Fmaster%2Fdocs%2Ftutorial.ipynb/get/\n",
      "2024-11-01 23:41:06,884 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fepurdom%2Fcobolt%2Fblob%2Fmaster%2Fdocs%2Ftutorial.ipynb/, Date de la dernière archive : 2024-11-01T22:05:09.115000+00:00\n",
      "2024-11-01 23:41:06,888 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fepurdom%2Fcobolt%2Fblob%2Fmaster%2Fdocs%2Ftutorial.ipynb/ à la date 2024-11-01T22:05:09.115000+00:00\n",
      "2024-11-01 23:41:06,889 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2FJinmiaoChenLab%2FSpaMosaic/get/\n",
      "2024-11-01 23:41:07,313 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2FJinmiaoChenLab%2FSpaMosaic/, Date de la dernière archive : 2024-11-01T22:05:19.459000+00:00\n",
      "2024-11-01 23:41:07,317 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2FJinmiaoChenLab%2FSpaMosaic/ à la date 2024-11-01T22:05:19.459000+00:00\n",
      "2024-11-01 23:41:07,318 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fspotify%2Fannoy/get/\n",
      "2024-11-01 23:41:07,798 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fspotify%2Fannoy/, Date de la dernière archive : 2024-11-01T22:05:19.620000+00:00\n",
      "2024-11-01 23:41:07,804 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fspotify%2Fannoy/ à la date 2024-11-01T22:05:19.620000+00:00\n",
      "2024-11-01 23:41:07,805 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fncbi%2FSilencerEnhancerPredict/get/\n",
      "2024-11-01 23:41:08,748 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fncbi%2FSilencerEnhancerPredict/, Date de la dernière archive : 2024-11-01T22:05:29.583000+00:00\n",
      "2024-11-01 23:41:08,753 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fncbi%2FSilencerEnhancerPredict/ à la date 2024-11-01T22:05:29.583000+00:00\n",
      "2024-11-01 23:41:08,754 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Flyli1013%2FDeepICSH/get/\n",
      "2024-11-01 23:41:09,736 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Flyli1013%2FDeepICSH/, Date de la dernière archive : 2024-11-01T22:05:29.591000+00:00\n",
      "2024-11-01 23:41:09,740 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Flyli1013%2FDeepICSH/ à la date 2024-11-01T22:05:29.591000+00:00\n",
      "2024-11-01 23:41:09,741 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fcuixj19%2FCREATE/get/\n",
      "2024-11-01 23:41:10,857 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fcuixj19%2FCREATE/, Date de la dernière archive : 2024-11-01T22:05:39.789000+00:00\n",
      "2024-11-01 23:41:10,862 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fcuixj19%2FCREATE/ à la date 2024-11-01T22:05:39.789000+00:00\n",
      "2024-11-01 23:41:10,863 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fuci-cbcl%2FDanQ/get/\n",
      "2024-11-01 23:41:11,060 - ERROR - Erreur lors de la vérification de https://github.com/uci-cbcl/DanQ : HTTPSConnectionPool(host='archive.softwareheritage.org', port=443): Max retries exceeded with url: /api/1/origin/https://github.com/uci-cbcl/DanQ/visits/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbad1d193d0>: Failed to establish a new connection: [Errno 12] Cannot allocate memory'))\n",
      "2024-11-01 23:41:11,277 - WARNING - Erreur lors de la soumission de https://github.com/uci-cbcl/DanQ : 403\n",
      "2024-11-01 23:41:14,283 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fhasindu2008%2Fslow5tools/get/\n",
      "2024-11-01 23:41:14,669 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fhasindu2008%2Fslow5tools/, Date de la dernière archive : 2024-11-01T22:05:44.338000+00:00\n",
      "2024-11-01 23:41:14,677 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fhasindu2008%2Fslow5tools/ à la date 2024-11-01T22:05:44.338000+00:00\n",
      "2024-11-01 23:41:14,678 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fnanoporetech%2Fvbz_compression/get/\n",
      "2024-11-01 23:41:15,083 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fnanoporetech%2Fvbz_compression/, Date de la dernière archive : 2024-11-01T22:05:49.849000+00:00\n",
      "2024-11-01 23:41:15,085 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fnanoporetech%2Fvbz_compression/ à la date 2024-11-01T22:05:49.849000+00:00\n",
      "2024-11-01 23:41:15,086 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fhasindu2008%2Fslow5lib/get/\n",
      "2024-11-01 23:41:15,475 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fhasindu2008%2Fslow5lib/, Date de la dernière archive : 2024-11-01T22:06:37.472000+00:00\n",
      "2024-11-01 23:41:15,478 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fhasindu2008%2Fslow5lib/ à la date 2024-11-01T22:06:37.472000+00:00\n",
      "2024-11-01 23:41:15,478 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fnanoporetech%2Fpod5-file-format/get/\n",
      "2024-11-01 23:41:15,864 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fnanoporetech%2Fpod5-file-format/, Date de la dernière archive : 2024-11-01T22:05:49.979000+00:00\n",
      "2024-11-01 23:41:15,867 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fnanoporetech%2Fpod5-file-format/ à la date 2024-11-01T22:05:49.979000+00:00\n",
      "2024-11-01 23:41:15,868 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2FWoldringLabMSU%2FEvoSeq-ML/get/\n",
      "2024-11-01 23:41:16,252 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2FWoldringLabMSU%2FEvoSeq-ML/, Date de la dernière archive : 2024-11-01T22:06:00.151000+00:00\n",
      "2024-11-01 23:41:16,256 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2FWoldringLabMSU%2FEvoSeq-ML/ à la date 2024-11-01T22:06:00.151000+00:00\n",
      "2024-11-01 23:41:16,257 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Faaronwtr%2FPertEval/get/\n",
      "2024-11-01 23:41:16,657 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Faaronwtr%2FPertEval/, Date de la dernière archive : 2024-11-01T22:06:00.124000+00:00\n",
      "2024-11-01 23:41:16,661 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Faaronwtr%2FPertEval/ à la date 2024-11-01T22:06:00.124000+00:00\n",
      "2024-11-01 23:41:16,662 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fgefeiwang%2FscMODAL/get/\n",
      "2024-11-01 23:41:17,052 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fgefeiwang%2FscMODAL/, Date de la dernière archive : 2024-11-01T22:06:10.239000+00:00\n",
      "2024-11-01 23:41:17,056 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fgefeiwang%2FscMODAL/ à la date 2024-11-01T22:06:10.239000+00:00\n",
      "2024-11-01 23:41:17,057 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Flmweber%2Fbenchmark-data-Levine-32-dim/get/\n",
      "2024-11-01 23:41:17,199 - ERROR - Erreur lors de la vérification de https://github.com/lmweber/benchmark-data-Levine-32-dim : 429 Client Error: Too Many Requests for url: https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Flmweber%2Fbenchmark-data-Levine-32-dim/get/\n",
      "2024-11-01 23:41:17,384 - WARNING - Erreur lors de la soumission de https://github.com/lmweber/benchmark-data-Levine-32-dim : 403\n",
      "2024-11-01 23:41:20,390 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fericcombiolab%2FHarmoDecon%2Ftree%2Fmain/get/\n",
      "2024-11-01 23:41:20,574 - ERROR - Erreur lors de la vérification de https://github.com/ericcombiolab/HarmoDecon/tree/main : 429 Client Error: Too Many Requests for url: https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fericcombiolab%2FHarmoDecon%2Ftree%2Fmain/get/\n",
      "2024-11-01 23:41:20,794 - WARNING - Erreur lors de la soumission de https://github.com/ericcombiolab/HarmoDecon/tree/main : 403\n",
      "2024-11-01 23:41:23,801 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2FEngelsI%2FClassiCOL/get/\n",
      "2024-11-01 23:41:24,466 - ERROR - Erreur lors de la vérification de https://github.com/EngelsI/ClassiCOL : 429 Client Error: Too Many Requests for url: https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2FEngelsI%2FClassiCOL/get/\n",
      "2024-11-01 23:41:25,006 - WARNING - Erreur lors de la soumission de https://github.com/EngelsI/ClassiCOL : 403\n",
      "2024-11-01 23:41:28,010 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Frolandfaure%2Fhairsplitter/get/\n",
      "2024-11-01 23:41:28,025 - ERROR - Erreur lors de la vérification de https://github.com/rolandfaure/hairsplitter : HTTPSConnectionPool(host='archive.softwareheritage.org', port=443): Max retries exceeded with url: /api/1/origin/https%3A%2F%2Fgithub.com%2Frolandfaure%2Fhairsplitter/get/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbaf39ccf10>: Failed to establish a new connection: [Errno 12] Cannot allocate memory'))\n",
      "2024-11-01 23:41:28,210 - WARNING - Erreur lors de la soumission de https://github.com/rolandfaure/hairsplitter : 403\n",
      "2024-11-01 23:41:31,214 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2FStatPhysBio%2Fhermes%2Ftree%2Fmain/get/\n",
      "2024-11-01 23:41:31,381 - ERROR - Erreur lors de la vérification de https://github.com/StatPhysBio/hermes/tree/main : 429 Client Error: Too Many Requests for url: https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2FStatPhysBio%2Fhermes%2Ftree%2Fmain/get/\n",
      "2024-11-01 23:41:31,536 - WARNING - Erreur lors de la soumission de https://github.com/StatPhysBio/hermes/tree/main : 403\n",
      "2024-11-01 23:41:34,542 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fdiyabc%2Fdiyabc%2Freleases%2Ftag%2Fv1.1.36/get/\n",
      "2024-11-01 23:41:34,725 - ERROR - Erreur lors de la vérification de https://github.com/diyabc/diyabc/releases/tag/v1.1.36 : 429 Client Error: Too Many Requests for url: https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fdiyabc%2Fdiyabc%2Freleases%2Ftag%2Fv1.1.36/get/\n",
      "2024-11-01 23:41:34,914 - WARNING - Erreur lors de la soumission de https://github.com/diyabc/diyabc/releases/tag/v1.1.36 : 403\n",
      "2024-11-01 23:41:37,922 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Ff-rousset%2F/get/\n",
      "2024-11-01 23:41:38,453 - ERROR - Erreur lors de la vérification de https://github.com/f-rousset/ : 429 Client Error: Too Many Requests for url: https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Ff-rousset%2F/get/\n",
      "2024-11-01 23:41:38,733 - WARNING - Erreur lors de la soumission de https://github.com/f-rousset/ : 403\n",
      "2024-11-01 23:41:41,738 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fadamjtaylor%2Fminiature%2F/get/\n",
      "2024-11-01 23:41:41,744 - ERROR - Erreur lors de la vérification de https://github.com/adamjtaylor/miniature/ : HTTPSConnectionPool(host='archive.softwareheritage.org', port=443): Max retries exceeded with url: /api/1/origin/https%3A%2F%2Fgithub.com%2Fadamjtaylor%2Fminiature%2F/get/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb01282e20>: Failed to establish a new connection: [Errno 12] Cannot allocate memory'))\n",
      "2024-11-01 23:41:41,914 - WARNING - Erreur lors de la soumission de https://github.com/adamjtaylor/miniature/ : 403\n",
      "2024-11-01 23:41:44,920 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2FCostaLab%2Fphlower/get/\n",
      "2024-11-01 23:41:45,406 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2FCostaLab%2Fphlower/, Date de la dernière archive : 2024-11-01T22:06:40.725000+00:00\n",
      "2024-11-01 23:41:45,411 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2FCostaLab%2Fphlower/ à la date 2024-11-01T22:06:40.725000+00:00\n",
      "2024-11-01 23:41:45,412 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fmarschall-lab%2FGFAffix/get/\n",
      "2024-11-01 23:41:45,831 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fmarschall-lab%2FGFAffix/, Date de la dernière archive : 2024-11-01T22:06:50.856000+00:00\n",
      "2024-11-01 23:41:45,835 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fmarschall-lab%2FGFAffix/ à la date 2024-11-01T22:06:50.856000+00:00\n",
      "2024-11-01 23:41:45,836 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fpangenome%2F/get/\n",
      "2024-11-01 23:41:46,184 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fpangenome%2F/, Date de la dernière archive : 2024-11-01T22:06:50.496000+00:00\n",
      "2024-11-01 23:41:46,189 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fpangenome%2F/ à la date 2024-11-01T22:06:50.496000+00:00\n",
      "2024-11-01 23:41:46,190 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fwaveygang%2Fwfmash/get/\n",
      "2024-11-01 23:41:46,715 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fwaveygang%2Fwfmash/, Date de la dernière archive : 2024-11-01T22:07:18.680000+00:00\n",
      "2024-11-01 23:41:46,718 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fwaveygang%2Fwfmash/ à la date 2024-11-01T22:07:18.680000+00:00\n",
      "2024-11-01 23:41:46,718 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fflatironinstitute%2FDeepFRI/get/\n",
      "2024-11-01 23:41:47,100 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fflatironinstitute%2FDeepFRI/, Date de la dernière archive : 2024-11-01T22:07:00.931000+00:00\n",
      "2024-11-01 23:41:47,105 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fflatironinstitute%2FDeepFRI/ à la date 2024-11-01T22:07:00.931000+00:00\n",
      "2024-11-01 23:41:47,106 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Fesm/get/\n",
      "2024-11-01 23:41:47,514 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Fesm/, Date de la dernière archive : 2024-11-01T22:07:01.254000+00:00\n",
      "2024-11-01 23:41:47,518 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Fesm/ à la date 2024-11-01T22:07:01.254000+00:00\n",
      "2024-11-01 23:41:47,519 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fwz-create%2FAptaDiff/get/\n",
      "2024-11-01 23:41:47,891 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fwz-create%2FAptaDiff/, Date de la dernière archive : 2024-11-01T22:07:11.090000+00:00\n",
      "2024-11-01 23:41:47,897 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fwz-create%2FAptaDiff/ à la date 2024-11-01T22:07:11.090000+00:00\n",
      "2024-11-01 23:41:47,898 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/http%3A%2F%2Fgithub.com%2FSheffieldML%2F/get/\n",
      "2024-11-01 23:41:48,280 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/http%3A%2F%2Fgithub.com%2FSheffieldML%2F/, Date de la dernière archive : 2024-11-01T22:07:10.855000+00:00\n",
      "2024-11-01 23:41:48,284 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/http%3A%2F%2Fgithub.com%2FSheffieldML%2F/ à la date 2024-11-01T22:07:10.855000+00:00\n",
      "2024-11-01 23:41:48,285 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fcox-labs%2FCoxLab_Bug_Reporting/get/\n",
      "2024-11-01 23:41:48,461 - ERROR - Erreur lors de la vérification de https://github.com/cox-labs/CoxLab_Bug_Reporting : 429 Client Error: Too Many Requests for url: https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fcox-labs%2FCoxLab_Bug_Reporting/get/\n",
      "2024-11-01 23:41:48,639 - WARNING - Erreur lors de la soumission de https://github.com/cox-labs/CoxLab_Bug_Reporting : 403\n",
      "2024-11-01 23:41:51,645 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2FJurgenCox%2Fmqtools7/get/\n",
      "2024-11-01 23:41:51,957 - ERROR - Erreur lors de la vérification de https://github.com/JurgenCox/mqtools7 : 429 Client Error: Too Many Requests for url: https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2FJurgenCox%2Fmqtools7/get/\n",
      "2024-11-01 23:41:52,311 - WARNING - Erreur lors de la soumission de https://github.com/JurgenCox/mqtools7 : 403\n",
      "2024-11-01 23:41:55,317 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fvandeplaslab%2Ffull_profile/get/\n",
      "2024-11-01 23:41:55,707 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fvandeplaslab%2Ffull_profile/, Date de la dernière archive : 2024-11-01T22:07:21.377000+00:00\n",
      "2024-11-01 23:41:55,712 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fvandeplaslab%2Ffull_profile/ à la date 2024-11-01T22:07:21.377000+00:00\n",
      "2024-11-01 23:41:55,713 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fmartinpacesa%2FBindCraft%2Fblob%2Fmain%2Fnotebooks%2FBindCraft.ipynb/get/\n",
      "2024-11-01 23:41:56,078 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fmartinpacesa%2FBindCraft%2Fblob%2Fmain%2Fnotebooks%2FBindCraft.ipynb/, Date de la dernière archive : 2024-11-01T22:07:31.301000+00:00\n",
      "2024-11-01 23:41:56,082 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fmartinpacesa%2FBindCraft%2Fblob%2Fmain%2Fnotebooks%2FBindCraft.ipynb/ à la date 2024-11-01T22:07:31.301000+00:00\n",
      "2024-11-01 23:41:56,083 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fmartinpacesa%2FBindCraft/get/\n",
      "2024-11-01 23:41:56,420 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fmartinpacesa%2FBindCraft/, Date de la dernière archive : 2024-11-01T22:07:32.796000+00:00\n",
      "2024-11-01 23:41:56,422 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2Fmartinpacesa%2FBindCraft/ à la date 2024-11-01T22:07:32.796000+00:00\n",
      "2024-11-01 23:41:56,423 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2FDIncalciLab%2Fsamurai/get/\n",
      "2024-11-01 23:41:56,769 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2FDIncalciLab%2Fsamurai/, Date de la dernière archive : 2024-11-01T22:07:41.701000+00:00\n",
      "2024-11-01 23:41:56,773 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2FDIncalciLab%2Fsamurai/ à la date 2024-11-01T22:07:41.701000+00:00\n",
      "2024-11-01 23:41:56,773 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2FDincalciLab%2Fsamurai/get/\n",
      "2024-11-01 23:41:57,119 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2FDincalciLab%2Fsamurai/, Date de la dernière archive : 2024-11-01T22:07:41.615000+00:00\n",
      "2024-11-01 23:41:57,124 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2FDincalciLab%2Fsamurai/ à la date 2024-11-01T22:07:41.615000+00:00\n",
      "2024-11-01 23:41:57,125 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2FDIncalciLab%2FSAMURAI_paper_scripts/get/\n",
      "2024-11-01 23:41:57,485 - INFO - Lien de l'archive : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2FDIncalciLab%2FSAMURAI_paper_scripts/, Date de la dernière archive : 2024-11-01T22:07:51.748000+00:00\n",
      "2024-11-01 23:41:57,489 - INFO - Dépôt déjà archivé : https://archive.softwareheritage.org/browse/origin/https%3A%2F%2Fgithub.com%2FDIncalciLab%2FSAMURAI_paper_scripts/ à la date 2024-11-01T22:07:51.748000+00:00\n",
      "2024-11-01 23:41:57,490 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fsarwanpasha%2FBinding-Site-Prediction/get/\n",
      "2024-11-01 23:41:57,910 - ERROR - Erreur lors de la vérification de https://github.com/sarwanpasha/Binding-Site-Prediction : 429 Client Error: Too Many Requests for url: https://archive.softwareheritage.org/api/1/origin/https://github.com/sarwanpasha/Binding-Site-Prediction/visits/\n",
      "2024-11-01 23:41:58,100 - WARNING - Erreur lors de la soumission de https://github.com/sarwanpasha/Binding-Site-Prediction : 403\n",
      "2024-11-01 23:42:01,106 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Flingxusb%2FPlasmidGPT/get/\n",
      "2024-11-01 23:42:01,287 - ERROR - Erreur lors de la vérification de https://github.com/lingxusb/PlasmidGPT : 429 Client Error: Too Many Requests for url: https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Flingxusb%2FPlasmidGPT/get/\n",
      "2024-11-01 23:42:01,466 - WARNING - Erreur lors de la soumission de https://github.com/lingxusb/PlasmidGPT : 403\n",
      "2024-11-01 23:42:04,470 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fbehzodcu%2Fstaffr/get/\n",
      "2024-11-01 23:42:04,646 - ERROR - Erreur lors de la vérification de https://github.com/behzodcu/staffr : 429 Client Error: Too Many Requests for url: https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fbehzodcu%2Fstaffr/get/\n",
      "2024-11-01 23:42:04,818 - WARNING - Erreur lors de la soumission de https://github.com/behzodcu/staffr : 403\n",
      "2024-11-01 23:42:07,831 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fzhengxinchang%2Fexcord-lr/get/\n",
      "2024-11-01 23:42:08,148 - ERROR - Erreur lors de la vérification de https://github.com/zhengxinchang/excord-lr : 429 Client Error: Too Many Requests for url: https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fzhengxinchang%2Fexcord-lr/get/\n",
      "2024-11-01 23:42:08,395 - WARNING - Erreur lors de la soumission de https://github.com/zhengxinchang/excord-lr : 403\n",
      "2024-11-01 23:42:11,401 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fryanlayer%2Fstix/get/\n",
      "2024-11-01 23:42:11,574 - ERROR - Erreur lors de la vérification de https://github.com/ryanlayer/stix : 429 Client Error: Too Many Requests for url: https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2Fryanlayer%2Fstix/get/\n",
      "2024-11-01 23:42:11,788 - WARNING - Erreur lors de la soumission de https://github.com/ryanlayer/stix : 403\n",
      "2024-11-01 23:42:14,794 - INFO - Vérification de l'URL : https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2FIBM%2FHestia-OOD/get/\n",
      "2024-11-01 23:42:14,950 - ERROR - Erreur lors de la vérification de https://github.com/IBM/Hestia-OOD : 429 Client Error: Too Many Requests for url: https://archive.softwareheritage.org/api/1/origin/https%3A%2F%2Fgithub.com%2FIBM%2FHestia-OOD/get/\n",
      "2024-11-01 23:42:15,146 - WARNING - Erreur lors de la soumission de https://github.com/IBM/Hestia-OOD : 403\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 163\u001b[0m\n\u001b[1;32m    160\u001b[0m                 logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL invalide trouvée : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdepot_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Exécuter la fonction pour traiter les dépôts\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m \u001b[43mprocess_repositories\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Validation finale des modifications et fermeture de la connexion à la base de données\u001b[39;00m\n\u001b[1;32m    166\u001b[0m database\u001b[38;5;241m.\u001b[39mcommit()\n",
      "Cell \u001b[0;32mIn[103], line 157\u001b[0m, in \u001b[0;36mprocess_repositories\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m             logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDépôt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdepot_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m soumis pour archivage.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m    156\u001b[0m         database\u001b[38;5;241m.\u001b[39mcommit()    \n\u001b[0;32m--> 157\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTIME_BETWEEN_REQUESTS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Gestion des URL invalides\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL invalide trouvée : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdepot_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configuration du logging pour afficher les informations de journalisation\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Fonction pour créer une nouvelle base de données avec les colonnes nécessaires\n",
    "def create_new_database(db_name=\"new_bioinformatics_articles.db\"):\n",
    "    # Connexion à la base de données SQLite\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Création de la table 'articles' si elle n'existe pas déjà, avec les colonnes requises\n",
    "        c.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS articles (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                title TEXT,\n",
    "                link TEXT,\n",
    "                doi TEXT,\n",
    "                date TEXT,\n",
    "                pdf_link TEXT,\n",
    "                abstract TEXT,\n",
    "                software_links TEXT,\n",
    "                is_article_processed INTEGER DEFAULT 0,\n",
    "                date_last_archive TEXT DEFAULT '',\n",
    "                url_archive TEXT DEFAULT ''\n",
    "            )\n",
    "        ''')\n",
    "        logging.info(f\"Nouvelle base de données '{db_name}' créée avec succès avec la table 'articles'.\")\n",
    "    except sqlite3.OperationalError as e:\n",
    "        # Gestion des erreurs potentielles lors de la création de la table\n",
    "        logging.error(f\"Erreur lors de la création de la base de données : {e}\")\n",
    "    finally:\n",
    "        # Validation des modifications et fermeture de la connexion\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "# Appel de la fonction pour créer la nouvelle base de données\n",
    "create_new_database()\n",
    "\n",
    "# Connexion à la base de données existante 'bioinformatics_articles.db'\n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor()\n",
    "\n",
    "# Ajouter les colonnes 'date_last_archive' et 'url_archive' si elles n'existent pas déjà\n",
    "try:\n",
    "    c.execute(\"ALTER TABLE articles ADD COLUMN date_last_archive TEXT DEFAULT '';\")\n",
    "    c.execute(\"ALTER TABLE articles ADD COLUMN url_archive TEXT DEFAULT '';\")\n",
    "    logging.info(\"Colonnes 'date_last_archive' et 'url_archive' ajoutées avec succès.\")\n",
    "except sqlite3.OperationalError as e:\n",
    "    # Gestion des erreurs si les colonnes existent déjà\n",
    "    logging.warning(f\"Les colonnes existent peut-être déjà : {e}\")\n",
    "\n",
    "# Token d'API Software Heritage pour les requêtes d'archivage\n",
    "TOKEN = \"votre_token_ici\"\n",
    "\n",
    "# Fonction pour vérifier si un dépôt est déjà archivé dans Software Heritage\n",
    "def check_archived(depot_url):\n",
    "    # URL de base pour vérifier l'archivage dans Software Heritage\n",
    "    base_url = \"https://archive.softwareheritage.org/api/1/origin/\"\n",
    "    encoded_url = urllib.parse.quote(depot_url, safe='')\n",
    "    check_url = f\"{base_url}{encoded_url}/get/\"\n",
    "\n",
    "    logging.info(f\"Vérification de l'URL : {check_url}\")\n",
    "    try:\n",
    "        # Requête pour vérifier si le dépôt est archivé\n",
    "        response = requests.get(check_url)\n",
    "        response.raise_for_status()\n",
    "        archive_info = response.json()\n",
    "\n",
    "        # Si l'URL des visites est présente, on peut vérifier la date d'archivage\n",
    "        if 'origin_visits_url' in archive_info:\n",
    "            origin_visits_url = archive_info['origin_visits_url']\n",
    "            visit_response = requests.get(origin_visits_url)\n",
    "            visit_response.raise_for_status()\n",
    "            visit_info = visit_response.json()\n",
    "\n",
    "            # Extraction de la dernière date d'archivage\n",
    "            if visit_info and 'date' in visit_info[0]:\n",
    "                last_visit = visit_info[0]\n",
    "                last_archive_date = last_visit['date']\n",
    "                archive_link = f\"https://archive.softwareheritage.org/browse/origin/{encoded_url}/\"\n",
    "                logging.info(f\"Lien de l'archive : {archive_link}, Date de la dernière archive : {last_archive_date}\")\n",
    "                return True, archive_link, last_archive_date\n",
    "            else:\n",
    "                logging.warning(\"Aucune date d'archivage trouvée.\")\n",
    "                return True, 'Lien non disponible', 'Date non disponible'\n",
    "\n",
    "        else:\n",
    "            logging.warning(\"Les informations d'archive ne contiennent pas de clé 'origin_visits_url'.\")\n",
    "            return True, 'Lien non disponible', 'Date non disponible'\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        # Gestion des erreurs de requête\n",
    "        logging.error(f\"Erreur lors de la vérification de {depot_url} : {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Fonction pour archiver un dépôt sur Software Heritage\n",
    "def archive_repo(depot_url):\n",
    "    # Définition du type de visite et encodage de l'URL\n",
    "    visit_type = \"git\"\n",
    "    encoded_url = urllib.parse.quote(depot_url, safe='')\n",
    "    save_url = f\"https://archive.softwareheritage.org/api/1/origin/save/{visit_type}/url/{encoded_url}/\"\n",
    "\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {TOKEN}'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Requête pour soumettre l'archivage\n",
    "        response = requests.post(save_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            logging.info(f\"Le dépôt {depot_url} a été soumis pour archivage avec succès.\")\n",
    "            return True\n",
    "        elif response.status_code == 429:\n",
    "            # Gestion de la limite de requêtes avec une pause de 3 secondes en cas de surcharge\n",
    "            logging.warning(\"Erreur 429 : Trop de requêtes. Pause de 3 secondes.\")\n",
    "            time.sleep(3)\n",
    "            return archive_repo(depot_url)\n",
    "        else:\n",
    "            logging.warning(f\"Erreur lors de la soumission de {depot_url} : {response.status_code}\")\n",
    "            return False\n",
    "    except requests.RequestException as e:\n",
    "        # Gestion des erreurs de requête\n",
    "        logging.error(f\"Erreur lors de la soumission de {depot_url} : {e}\")\n",
    "        return False\n",
    "\n",
    "# Limites de requêtes par heure pour éviter la surcharge du serveur\n",
    "MAX_REQUESTS_PER_HOUR = 1200\n",
    "TIME_BETWEEN_REQUESTS = 3600 / MAX_REQUESTS_PER_HOUR\n",
    "\n",
    "# Lecture des URLs des dépôts et gestion de l'archivage\n",
    "def process_repositories():\n",
    "    # Sélection des articles dont les informations d'archive sont vides\n",
    "    c.execute(\"SELECT software_links FROM articles WHERE software_links IS NOT NULL AND date_last_archive = '' AND url_archive = ''\")\n",
    "    rows = c.fetchall()\n",
    "\n",
    "    for row in rows:\n",
    "        depot_urls = row[0].split(',')\n",
    "        \n",
    "        for depot_url in depot_urls:\n",
    "            depot_url = depot_url.strip()\n",
    "\n",
    "            if depot_url and depot_url.startswith(\"http\"):\n",
    "                # Vérifier si le dépôt est déjà archivé\n",
    "                is_archived, archive_link, last_archive_date = check_archived(depot_url)\n",
    "\n",
    "                if is_archived:\n",
    "                    # Mise à jour des informations d'archivage si le dépôt est déjà archivé\n",
    "                    c.execute(\"UPDATE articles SET date_last_archive = ?, url_archive = ? WHERE software_links LIKE ?\",\n",
    "                              (last_archive_date, archive_link, f'%{depot_url}%'))\n",
    "                    database.commit()\n",
    "                    logging.info(f\"Dépôt déjà archivé : {archive_link} à la date {last_archive_date}\")\n",
    "                else:\n",
    "                    # Soumettre le dépôt pour archivage s'il n'est pas déjà archivé\n",
    "                    if archive_repo(depot_url):\n",
    "                        logging.info(f\"Dépôt {depot_url} soumis pour archivage.\") \n",
    "                    database.commit()    \n",
    "                    time.sleep(TIME_BETWEEN_REQUESTS)\n",
    "            else:\n",
    "                # Gestion des URL invalides\n",
    "                logging.warning(f\"URL invalide trouvée : {depot_url}\")\n",
    "\n",
    "# Exécuter la fonction pour traiter les dépôts\n",
    "process_repositories()\n",
    "\n",
    "# Validation finale des modifications et fermeture de la connexion à la base de données\n",
    "database.commit()\n",
    "database.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR ME : check la table sur le jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connexion à la base de données SQLite\n",
    "database = sqlite3.connect('bioinformatics_articles.db')\n",
    "c = database.cursor()\n",
    "\n",
    "# Fonction pour afficher les attributs de la table articles\n",
    "def display_articles():\n",
    "    # Exécuter une requête pour récupérer toutes les colonnes et enregistrements de la table articles\n",
    "    c.execute(\"SELECT * FROM articles\")\n",
    "    \n",
    "    # Récupérer les noms de colonnes\n",
    "    column_names = [description[0] for description in c.description]\n",
    "    \n",
    "    # Afficher les noms des colonnes\n",
    "    print(f\"{' | '.join(column_names)}\")\n",
    "    print(\"-\" * (len(column_names) * 4))  # Ligne de séparation\n",
    "    \n",
    "    # Récupérer et afficher tous les enregistrements\n",
    "    for row in c.fetchall():\n",
    "        print(' | '.join(map(str, row)))\n",
    "\n",
    "# Appeler la fonction pour afficher les articles\n",
    "display_articles()\n",
    "\n",
    "# Fermer la connexion à la base de données\n",
    "database.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le faire en cas de soucis and need tout recommencer \n",
    "#c.execute(\"DROP TABLE IF EXISTS articles\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
