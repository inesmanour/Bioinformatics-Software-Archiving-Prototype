{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #004C47;\">Installation des bibliothèques nécessaires </h2>\n",
    "<div style=\"border: 2px solid #01796F; background-color: #E0F2F1; color: #01796F; padding: 15px; border-radius: 5px; font-size: 16px;\">\n",
    "    <strong>\n",
    "    \n",
    "**pip install beautifulsoup4**\n",
    "\n",
    "**pip install requests**\n",
    "\n",
    "**pip install PyPDF2**\n",
    "\n",
    "**pip install pdfplumber**   </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pdfplumber\n",
    "import re\n",
    "import time\n",
    "import urllib.parse\n",
    "import io\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import PyPDF2\n",
    "from io import BytesIO\n",
    "import os\n",
    "from threading import Timer\n",
    "from concurrent.futures import ThreadPoolExecutor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #004C47;\">Chargement et rechargement du module tools</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tools\n",
    "\n",
    "importlib.reload(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #004C47;\">1. Scraper les articles bioRxiv liés à la bioinformatique</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #01796F; background-color: #E0F2F1; color: #01796F; padding: 15px; border-radius: 5px; font-size: 16px;\">\n",
    "    <strong>Objectif:</strong> Ce script permet d'archiver automatiquement les dépôts logiciels non encore archivés dans la base Software Heritage.Récupérer les articles publiés sur bioRxiv dans le domaine de la bioinformatique, en collectant les informations essentielles pour une analyse et un traitement ultérieurs.\n",
    "\n",
    "Informations collectées :\n",
    "\n",
    "\t•Titre de l’article : Titre complet décrivant le sujet de recherche.\n",
    "\t•Résumé (abstract) : Vue d’ensemble concise du contenu scientifique.\n",
    "\t•Auteurs : Liste des auteurs, ordonnée telle qu’elle apparaît sur bioRxiv.\n",
    "\t•Date de publication : Date de mise en ligne pour le suivi des recherches.\n",
    "\t•DOI : Identifiant unique facilitant la référence et l’accès.\n",
    "\t•URL de l’article : Lien direct vers la page de l’article sur bioRxiv.\n",
    "\t•Mots-clés : Mots-clés associés pour décrire le domaine ou sujet de recherche.\n",
    "\t•URLs des dépôts logiciels : Liens vers les dépôts de code (GitHub, GitLab) associés aux ressources logicielles.\n",
    "\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools\n",
    "\n",
    "def scrape_articles(total_pages=10):\n",
    "    \"\"\"Scrape uniquement les nouveaux articles bioRxiv et les enregistre dans la table 'articles' de la nouvelle base.\"\"\"\n",
    "    url = 'https://www.biorxiv.org'\n",
    "    conn, c = tools.create_db(\"bioinformatics_article.db\")  # Utilise la nouvelle base de données\n",
    "\n",
    "    for page in range(1, total_pages + 1):\n",
    "        print(f\"Scraping page {page} sur {total_pages}\")\n",
    "        response = requests.get(f'{url}/collection/bioinformatics?page={page}')\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        articles = soup.find_all('a', class_='highwire-cite-linked-title')\n",
    "\n",
    "        for article in articles:\n",
    "            title = article.text.strip()\n",
    "            link = url + article['href']\n",
    "\n",
    "            article_response = requests.get(link)\n",
    "            article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "\n",
    "            doi_tag = article_soup.find('meta', {'name': 'citation_doi'})\n",
    "            doi = doi_tag['content'] if doi_tag else 'DOI non disponible'\n",
    "\n",
    "            # Vérifie si l'article est déjà dans la base de données\n",
    "            c.execute(\"SELECT * FROM articles WHERE doi = ?\", (doi,))\n",
    "            if c.fetchone():\n",
    "                print(f\"L'article avec DOI {doi} existe déjà dans la base. Arrêt du scraping.\")\n",
    "                tools.close_db(conn)\n",
    "                return  # Sortir de la fonction car tous les articles suivants existent déjà\n",
    "\n",
    "            # Si le DOI est nouveau, nous récupérons les autres informations\n",
    "            date_tag = article_soup.find('meta', {'name': 'citation_publication_date'})\n",
    "            date = date_tag['content'] if date_tag else 'Date non disponible'\n",
    "            if date != 'Date non disponible':\n",
    "                date = date.replace('/', '-')\n",
    "                date = datetime.strptime(date, '%Y-%m-%d').date().isoformat()\n",
    "\n",
    "            pdf_links = article_soup.find_all('a')\n",
    "            pdf_link = next((url + pdf['href'] for pdf in pdf_links if 'PDF' in pdf.text and pdf['href'].endswith('.pdf')), 'Lien PDF non disponible')\n",
    "\n",
    "            # Extraire l'abstract et nettoyer les balises HTML\n",
    "            abstract_tag = article_soup.find('meta', {'name': 'citation_abstract'})\n",
    "            if abstract_tag:\n",
    "                raw_abstract = abstract_tag['content']\n",
    "                clean_abstract = BeautifulSoup(raw_abstract, \"html.parser\").get_text()  # Enlever les balises HTML\n",
    "            else:\n",
    "                clean_abstract = 'Abstract non disponible'\n",
    "\n",
    "            # Insertion du nouvel article dans la base de données avec clean_abstract\n",
    "            c.execute('''\n",
    "                INSERT INTO articles (title, link, doi, date, pdf_link, abstract)\n",
    "                VALUES (?, ?, ?, ?, ?, ?)\n",
    "            ''', (title, link, doi, date, pdf_link, clean_abstract))  # Utiliser clean_abstract ici\n",
    "            print(f\"Article ajouté : {title}\")\n",
    "            conn.commit()  # Valide la transaction immédiatement\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "    # Ferme la base de données\n",
    "    tools.close_db(conn)\n",
    "\n",
    "# Exécuter la fonction de scraping\n",
    "scrape_articles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #004C47;\">2. Extraction des URLs des Dépôts Logiciels</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #01796F; background-color: #E0F2F1; color: #01796F; padding: 15px; border-radius: 5px; font-size: 16px;\">\n",
    "    <strong>Objectif : </strong> Extraire automatiquement les liens vers les dépôts logiciels (GitHub, GitLab) mentionnés dans les articles bioRxiv liés à la bioinformatique. Ce script parcourt les PDF et les résumés (abstracts) des articles pour identifier et collecter ces URLs de dépôt, qui seront ensuite utilisés pour archivage ou analyse.\n",
    "\n",
    " </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def extract_repository_links(db_name=\"bioinformatics_article.db\"):\n",
    "    conn, c = tools.create_db(db_name)\n",
    "    c.execute(\"SELECT id, title, abstract, pdf_link FROM articles WHERE is_article_processed = 0\")\n",
    "    articles = c.fetchall()\n",
    "\n",
    "    def process_article(article):\n",
    "        article_id, title, abstract, pdf_link = article\n",
    "        logging.info(f\"Traitement de l'article '{title}' (ID: {article_id})\")\n",
    "\n",
    "        # Extraction des liens de dépôt\n",
    "        repo_links = set()\n",
    "        if abstract:\n",
    "            repo_links.update(tools.extract_repository_urls(abstract))\n",
    "        if pdf_link and pdf_link != 'Lien PDF non disponible':\n",
    "            repo_links.update(tools.extract_links_from_pdf(pdf_link))\n",
    "\n",
    "        if repo_links:\n",
    "            logging.info(f\"{len(repo_links)} liens de dépôt trouvés pour l'article '{title}'\")\n",
    "            for repo_url in repo_links:\n",
    "                code_repo_id = tools.insert_code_repository(conn, repo_url)\n",
    "                if code_repo_id:\n",
    "                    tools.link_article_to_repo(conn, article_id, code_repo_id)\n",
    "            c.execute(\"UPDATE articles SET contains_valid_repo_link = 1 WHERE id = ?\", (article_id,))\n",
    "        else:\n",
    "            logging.info(f\"Aucun lien de dépôt valide trouvé pour l'article '{title}'\")\n",
    "\n",
    "        # Mise à jour pour indiquer que l'article a été traité\n",
    "        c.execute(\"UPDATE articles SET is_article_processed = 1 WHERE id = ?\", (article_id,))\n",
    "        conn.commit()\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Utiliser le multithreading pour traiter les articles en parallèle\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        executor.map(process_article, articles)\n",
    "\n",
    "    tools.close_db(conn)\n",
    "\n",
    "# Exécuter la fonction\n",
    "extract_repository_links()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #004C47;\">3. Archivage via Software Heritage</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #01796F; background-color: #E0F2F1; color: #01796F; padding: 15px; border-radius: 5px; font-size: 16px;\">\n",
    "    <strong>Objectif : </strong>  Assurer l’archivage à long terme des dépôts logiciels associés aux articles via l’API de Software Heritage.\n",
    "\n",
    "\t•Vérification : Pour chaque dépôt, le script vérifie s’il est déjà archivé.\n",
    "\t•Soumission automatique : Si non archivé, une demande d’archivage est soumise automatiquement.\n",
    "\t•Mise à jour : La base de données est mise à jour avec les liens et dates d’archivage.\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def archive_repositories(db_name=\"bioinformatics_article.db\"):\n",
    "    # Création initiale de la base de données sans l'utiliser directement dans les threads\n",
    "    conn, c = tools.create_db(db_name)\n",
    "    c.execute(\"SELECT code_repo_id, code_repo_url FROM code_repositories WHERE is_archived_in_swh = 0\")\n",
    "    repos = c.fetchall()\n",
    "    tools.close_db(conn)  # Fermer la connexion principale ici, pour ne pas l'utiliser dans les threads\n",
    "\n",
    "    # Passer `db_name` au lieu de `conn` à chaque thread\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [executor.submit(process_repo, db_name, repo) for repo in repos]\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Erreur lors de l'archivage d'un dépôt : {e}\")\n",
    "\n",
    "def process_repo(db_name, repo):\n",
    "    conn, c = tools.create_db(db_name)  # Nouvelle connexion pour chaque thread\n",
    "    code_repo_id, code_repo_url = repo\n",
    "    logging.info(f\"Vérification de l'archivage pour le dépôt : {code_repo_url}\")\n",
    "\n",
    "    for attempt in range(tools.MAX_RETRIES):\n",
    "        is_archived, archive_date, archive_link = tools.check_archived(code_repo_url)\n",
    "        \n",
    "        if is_archived is None:\n",
    "            logging.error(f\"Impossible de vérifier l'archive pour le dépôt {code_repo_url} (tentative {attempt + 1}/{tools.MAX_RETRIES}).\")\n",
    "            wait_time = tools.WAIT_INTERVAL * (attempt + 1)\n",
    "            logging.info(f\"Attente de {wait_time} secondes avant de réessayer...\")\n",
    "            time.sleep(wait_time)  # Délai progressif avant de réessayer\n",
    "            continue  # Réessaie après l'attente\n",
    "\n",
    "        if is_archived:\n",
    "            logging.info(f\"Dépôt déjà archivé. Mise à jour des informations pour le dépôt {code_repo_url}\")\n",
    "            c.execute(\"\"\"\n",
    "                UPDATE code_repositories\n",
    "                SET is_archived_in_swh = 1, swh_archive_link = ?, swh_date_last_archive = ?\n",
    "                WHERE code_repo_id = ?\n",
    "            \"\"\", (archive_link, archive_date, code_repo_id))\n",
    "        else:\n",
    "            archived = tools.archive_repo(code_repo_url)\n",
    "            if archived:\n",
    "                logging.info(f\"Dépôt soumis pour archivage : {code_repo_url}\")\n",
    "                c.execute(\"UPDATE code_repositories SET is_archived_in_swh = 2 WHERE code_repo_id = ?\", (code_repo_id,))\n",
    "            else:\n",
    "                logging.error(f\"Échec de la soumission pour le dépôt {code_repo_url}\")\n",
    "\n",
    "        conn.commit()\n",
    "        break  # Sortir de la boucle si succès\n",
    "\n",
    "    tools.close_db(conn)  # Fermez la connexion proprement\n",
    "\n",
    "# Exécuter la fonction principale\n",
    "archive_repositories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #004C47;\">4. Re-vérification de l’archivage pour les dépôts soumis</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #01796F; background-color: #E0F2F1; color: #01796F; padding: 15px; border-radius: 5px; font-size: 16px;\">\n",
    "    <strong>Objectif : </strong>  Vérifier l’état d’archivage des dépôts soumis pour archivage auprès de Software Heritage.\n",
    "\n",
    "\t•Vérification périodique : Ce script revisite les dépôts soumis (ceux en attente d’archivage) pour voir si l’archivage a été complété.\n",
    "\t•Mise à jour : Si un dépôt est archivé avec succès, le script enregistre le lien et la date d’archivage dans la base de données.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "import tools\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def recheck_archived_repositories(db_name=\"bioinformatics_article.db\"):\n",
    "    # Création initiale de la base de données pour récupérer les dépôts sans utiliser cette connexion dans les threads\n",
    "    conn, c = tools.create_db(db_name)\n",
    "    c.execute(\"SELECT code_repo_id, code_repo_url FROM code_repositories WHERE is_archived_in_swh = 2\")\n",
    "    repos = c.fetchall()\n",
    "    tools.close_db(conn)  # Fermer la connexion principale ici, pour ne pas l'utiliser dans les threads\n",
    "\n",
    "    # Utilisation du ThreadPoolExecutor avec un maximum de 5 threads\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_to_repo = {executor.submit(process_repo, db_name, repo): repo for repo in repos}\n",
    "        for future in as_completed(future_to_repo):\n",
    "            repo = future_to_repo[future]\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Erreur de traitement dans le thread pour le dépôt {repo[1]}: {e}\")\n",
    "\n",
    "def process_repo(db_name, repo):\n",
    "    # Chaque thread ouvre sa propre connexion à la base de données\n",
    "    conn, c = tools.create_db(db_name)\n",
    "    code_repo_id, code_repo_url = repo\n",
    "    logging.info(f\"Re-vérification de l'archivage pour le dépôt : {code_repo_url}\")\n",
    "\n",
    "    try:\n",
    "        is_archived, archive_date, archive_link = tools.check_archived(code_repo_url)\n",
    "        if is_archived:\n",
    "            logging.info(f\"Dépôt archivé après soumission. Mise à jour pour le dépôt {code_repo_url}\")\n",
    "            c.execute(\n",
    "                \"UPDATE code_repositories SET is_archived_in_swh = 1, swh_archive_link = ?, swh_date_last_archive = ? WHERE code_repo_id = ?\",\n",
    "                (archive_link, archive_date, code_repo_id)\n",
    "            )\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erreur lors de la re-vérification du dépôt {code_repo_url}: {e}\")\n",
    "    finally:\n",
    "        # Fermez la connexion proprement\n",
    "        tools.close_db(conn)\n",
    "\n",
    "# Exécuter la fonction\n",
    "recheck_archived_repositories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #004C47;\">Affichage des tables et colonnes de la base de données </h2>\n",
    "\n",
    "<div style=\"border: 2px solid #01796F; background-color: #E0F2F1; color: #01796F; padding: 15px; border-radius: 5px; font-size: 16px;\">\n",
    "    <strong>Objectif : </strong> Visualiser la structure de la base de données en restant sur le fichier jupyter pour confirmer la présence des tables attendues et des colonnes associées.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def show_tables():\n",
    "    \"\"\"Affiche le contenu des tables articles, code_repositories et articles_code_repositories \n",
    "    dans la base de données bioinformatics_article.db avec les noms des colonnes.\"\"\"\n",
    "    \n",
    "    # Connexion à la base de données\n",
    "    conn = sqlite3.connect(\"bioinformatics_article.db\")\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Fonction pour afficher le contenu d'une table\n",
    "    def show_table_content(table_name):\n",
    "        print(f\"\\nContenu de la table '{table_name}':\")\n",
    "        c.execute(f\"SELECT * FROM {table_name}\")\n",
    "        columns = [description[0] for description in c.description]\n",
    "        print(columns)  # Affiche les noms des colonnes\n",
    "        rows = c.fetchall()\n",
    "        for row in rows:\n",
    "            print(row)\n",
    "\n",
    "    # Afficher le contenu des tables\n",
    "    show_table_content(\"articles\")\n",
    "    show_table_content(\"code_repositories\")\n",
    "    show_table_content(\"articles_code_repositories\")\n",
    "\n",
    "    # Fermer la connexion\n",
    "    conn.close()\n",
    "\n",
    "# Exécuter la fonction pour afficher le contenu des tables\n",
    "show_tables()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
